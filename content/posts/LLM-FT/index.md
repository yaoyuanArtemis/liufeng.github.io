---
title: "LLM - æ¨¡å‹å¾®è°ƒ"
date: 2025-08-10T13:13:14+08:00
draft: false
description: LLMæ¨¡å‹å¾®è°ƒæ¦‚è¿°
toc: true
authors:
  - example-author
tags:
  - LLM
  - äººå·¥æ™ºèƒ½
categories:
    - LLM
    - GPT
    - Machine Learning
---

## ç§ç±»

å¾®è°ƒä¸»è¦åŒ…æ‹¬ä¸‰ç§ç±»å‹ï¼š

* SFTï¼ˆæœ‰ç›‘ç£å¾®è°ƒï¼‰

  * Supervised Fine-Tuning

  * é€šè¿‡äººå·¥æ ‡æ³¨çš„æ•°æ®ï¼Œè¿›ä¸€æ­¥è®­ç»ƒ**é¢„è®­ç»ƒæ¨¡å‹ï¼Œ**&#x8BA9;æ¨¡å‹èƒ½å¤Ÿèƒœä»»åœ¨ç‰¹å®šé¢†åŸŸ

  * é™¤äº†æœ‰ç›‘ç£å¾®è°ƒï¼Œè¿˜åŒ…æ‹¬â€œæ— ç›‘ç£å¾®è°ƒâ€ï¼Œâ€œè‡ªç›‘ç£å¾®è°ƒâ€

  * å¾®è°ƒç®—æ³•åˆ†ç±»ï¼š

    * å…¨å‚æ•°å¾®è°ƒï¼š

      * ä¼˜ç‚¹ï¼šå¯ä»¥è·å¾—æœ€ä½³æ€§èƒ½

      * ç¼ºç‚¹ï¼šéœ€è¦è¾ƒå¤§è®¡ç®—æ€§èƒ½

    * éƒ¨åˆ†å‚æ•°å¾®è°ƒï¼š

      * ä¼˜ç¼ºç‚¹ï¼šåä¹‹

      * case:LoRA

* RLFHï¼ˆå¼ºåŒ–å­¦ä¹ ï¼‰

  * DPOï¼ˆDirect Preference Optimizationï¼‰é€šè¿‡äººç±»**ä¸»åŠ¨é€‰æ‹©**ï¼Œç›´æ¥ä¼˜åŒ–æ¨¡å‹ï¼›è°ƒæ•´å¹…åº¦å¤§

  * PPOï¼ˆProximal Policy Optimizationï¼‰é€šè¿‡**ç‚¹èµï¼Œç‚¹è¸©**æ¥æ¸è¿›å¼è°ƒæ•´æ¨¡å‹ï¼Œ

* RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰

  * å°†æ–‡æœ¬ç”Ÿæˆå’Œå¤–éƒ¨ä¿¡æ¯æ£€ç´¢ç»“åˆï¼Œ***å®æ—¶*è·å–å¤–éƒ¨ä¿¡æ¯å’Œæœ€æ–°ä¿¡æ¯**

RAGå’ŒSFTçš„åŒºåˆ«ä¸è”ç³»ï¼š

## å¾®è°ƒå‚æ•°

* æ¡†æ¶ï¼š[LLaMA Factory](https://llamafactory.readthedocs.io/zh-cn/latest/)

* ç®—æ³•ï¼šLora

* åŸºåº§æ¨¡å‹ï¼šDeepseek-R1-Distill-Qwen-1.5B

* webæ¡†æ¶ï¼šFastAPI

## LoRA

[ LLM - LoRA\QLoRA](https://pcnqdohorvbp.feishu.cn/wiki/D697w4hcqiC1GGkH3kkcebzdnOf)

## SFTæ•´ä½“æ­¥éª¤

### ç§Ÿç”¨äº‘GPU



![](images/image-11.png)

### SSHç™»é™†

![ç™»é™†æˆåŠŸä¼šæœ‰æç¤º](images/image-10.png)

### ä¸‹è½½å®‰è£…æ¨¡å‹è®­ç»ƒæ¡†æ¶

```bash
git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git
```

ä¸‹è½½å¯èƒ½ä¼šè¢«ğŸ§±

```bash
git clone --depth 1 https://hub.gitmirror.com/https://github.com/hiyouga/LLaMA-Factory.git
```

### åˆ›å»ºcondaç¯å¢ƒ

```bash
conda create -n llama-factory python=3.10
```

### å®‰è£…llamafactoryå¹¶éªŒè¯

```bash
llamafactory-cli version
```

### llama-cliå¯è§†åŒ–

```bash
llamafactory-cli web-ui    
```

![](images/image.png)

### hugging-faceä¸‹è½½æ¨¡å‹

```bash
// å­˜å‚¨æ¨¡å‹æ–‡ä»¶å¤¹
mkdir hugging-face

// ä¿®æ”¹hugging-faceé•œåƒæº
export HF_ENDPOINT=https://hf-mirror.com

// ä¿®æ”¹æ¨¡å‹ä¸‹è½½ä½ç½®
export HF_HOME=/root/autodl-tmp/hugging-face

// è¿™æ˜¯ä¸´æ—¶é…ç½®ï¼Œå¦‚æœè¦æ°¸ä¹…å†™å…¥è¿˜å¾—æ·»åŠ åˆ°~/.bashrc ~/.zshrc

echo $HF_HOME
echo $HF_ENDPOITNT

// å®‰è£…hugging-faceä¸‹è½½å·¥å…·
pip install -U huggingface_hub
```

### UIåŠ è½½æ¨¡å‹å¹¶éªŒè¯

![](images/image-1.png)

![](images/image-2.png)

### å‡†å¤‡è®­ç»ƒæ•°æ®

* æŒ‰ç…§è§„å®šæ ¼å¼

æ–‡ä»¶è·¯å¾„ä¹Ÿåœ¨dataç›®å½•ä¸‹

![](images/image-3.png)

* dataset\_info.jsonæŒ‡å‡ºæ–°å¢æ•°æ®è·¯å¾„

  ```bash
  "magic_conch": {
  "file_name": "magic_conch.json"
  },
  ```

![](images/image-4.png)

### UIå¾®è°ƒå‚æ•°è®¾ç½®

![](images/image-5.png)



å­¦ä¹ ç‡ï¼šæ¨¡å‹æ›´æ–°æ—¶æƒé‡æ”¹å˜å¹…åº¦

å­¦ä¹ ç‡å¤§ - é”™è¿‡æœ€ä¼˜è§£

å­¦ä¹ ç‡å° - å­¦ä¹ å¾ˆæ…¢

è®­ç»ƒè®ºæ•°ï¼š

å°ï¼›æ¬ æ‹Ÿåˆ

å¤§ï¼šè¿‡æ‹Ÿåˆ

æœ€å¤§æ¢¯åº¦é™åˆ¶ï¼š

æ¢¯åº¦é˜ˆå€¼

æœ€å¤§æ ·æœ¬æ•°ï¼š

è®­ç»ƒæ•°æ®ä¸­å–æ ·ä¸€éƒ¨åˆ†

æˆªæ–­é•¿åº¦ï¼š

ä¸Šä¸‹æ–‡é•¿åº¦

æ‰¹å¤„ç†å¤§å°

æ¢¯åº¦ç´¯è®¡

å­¦ä¹ ç‡è°ƒèŠ‚å™¨

![](images/image-6.png)

### å¯¼å‡ºæ¨¡å‹

æŠŠä¸è®­ç»ƒå¥½çš„æ¨¡å‹å’Œåˆå§‹æ¨¡å‹åˆå¹¶å¹¶å¯¼å‡ºä¾›ä½¿ç”¨

### æ¨¡å‹éƒ¨ç½²å’Œæ¥å£æš´éœ²

1. åˆ›å»ºæ–°çš„ç¯å¢ƒæ¥åšæ¨¡å‹éƒ¨ç½²

   å’Œä¸Šé¢çš„llamafactoryç¯å¢ƒéš”ç¦»

   ```bash
   // åˆ›å»ºfastApiç¯å¢ƒ
   conda create -n fastApi python=3.10

   // æ¿€æ´»ç¯å¢ƒ
   conda activate fstApi

   // å®‰è£…ä¾èµ–
   conda install -c conda-forge fastapi uvicorn transformers pytorch
   pip install safetensors sentencepiece protobuf
   ```

2. å¯åŠ¨åç«¯æœåŠ¡

   ```bash
   from fastapi import FastAPI
   from transformers import AutoModelForCausalLM, AutoTokenizer
   import torch

   app = FastAPI()

   # æ¨¡å‹è·¯å¾„
   model_path = "/root/autodl-tmp/Models/deepseek-r1-1.5b-merged"

   # åŠ è½½ tokenizer ï¼ˆåˆ†è¯å™¨ï¼‰
   tokenizer = AutoTokenizer.from_pretrained(model_path)

   # åŠ è½½æ¨¡å‹å¹¶ç§»åŠ¨åˆ°å¯ç”¨è®¾å¤‡ï¼ˆGPU/CPUï¼‰
   device = "cuda" if torch.cuda.is_available() else "cpu"
   model = AutoModelForCausalLM.from_pretrained(model_path).to(device)

   @app.get("/generate")
   async def generate_text(prompt: str):
       # ä½¿ç”¨ tokenizer ç¼–ç è¾“å…¥çš„ prompt
       inputs = tokenizer(prompt, return_tensors="pt").to(device)
       
       # ä½¿ç”¨æ¨¡å‹ç”Ÿæˆæ–‡æœ¬
       outputs = model.generate(inputs["input_ids"], max_length=150)
       
       # è§£ç ç”Ÿæˆçš„è¾“å‡º
       generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
       
       return {"generated_text": generated_text}
   ```

## RAGéƒ¨ç½²

RAGå’Œæ¨¡å‹å¾®è°ƒéƒ½æ˜¯ä¸ºäº†è§£å†³å¤§æ¨¡å‹çš„**å¹»è§‰**é—®é¢˜

åœ¨ç”Ÿæˆå›ç­”ä¹‹å‰ä¼šæå‰åœ¨å¤–éƒ¨çŸ¥è¯†åº“ä¸­æ£€ç´¢

### æœ¬åœ°éƒ¨ç½²å…¨æµç¨‹

1. ä¸‹è½½ollamaï¼Œé€šè¿‡ollamaå°†DeepSeekæ¨¡å‹ä¸‹è½½åˆ°æœ¬åœ°

2. ä¸‹è½½RAGflowå’ŒDockerï¼Œé€šè¿‡Dockeréƒ¨ç½²RAGflow

3. åœ¨RAGflowä¸­éƒ¨ç½²ä¸ªäººçŸ¥è¯†åº“å¹¶å®ç°åŸºäºä¸ªäººçŸ¥è¯†åº“çš„å›ç­”

### æ­¥éª¤

* å®‰è£…ollamaä»¥åŠå¯¹åº”åŸºåº§æ¨¡å‹

![](images/image-7.png)

è®°å¾—é…ç½®ç¯å¢ƒå˜é‡ï¼ˆç•¥ï¼‰

* ä¸‹è½½RAGflow



RAGflowä¼šè¿è¡Œåœ¨Dockerå®¹å™¨ä¸­ä¸­

```bash
$ docker compose -f docker-compose.yml up -d
```

![](images/image-8.png)

### è®¿é—®RAGflow

é€šè¿‡localhost:80æ‰“å¼€RAGflowå‰ç«¯é¡µé¢

![](images/image-9.png)

