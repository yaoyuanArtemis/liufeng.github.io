---
title: LLM-å¤§æ¨¡å‹
description: LLMåŸºç¡€ç¯‡æ€»ç»“
toc: true
authors:
  - example-author
tags:
  - LLM
  - äººå·¥æ™ºèƒ½
categories:
    - LLM
    - GPT
    - Machine Learning
date: '2021-03-31T13:11:22+08:00'
lastmod: '2021-03-31T13:11:22+08:00'
featuredImage:
draft: false
---

ä¸€è¨€ä»¥è”½ä¹‹ï¼Œä¸€ä¸ªLLMæ¨¡å‹å°±æ˜¯ä¸€ä¸ªæ¦‚ç‡æ•°æ®åº“ã€‚å®ƒä¸ºä»»ä½•ç»™å®šå­—ç¬¦çš„ä¸Šä¸‹æ–‡å­—ç¬¦åˆ†é…ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒ

## LLM-åŸç†ğŸ˜ˆ

### èƒŒæ™¯

åœ¨LLMå‡ºç°ä¹‹å‰ï¼Œæœºå™¨å¯¹ç¥ç»ç½‘ç»œçš„è®­ç»ƒå—é™äºç›¸å¯¹è¾ƒå°çš„æ•°æ®é›†ï¼Œå¯¹ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›éå¸¸æœ‰é™

Google Brainå›¢é˜Ÿåœ¨2017å¹´å‘å¸ƒäº†ã€ŠAttetion is all your needã€‹åå¼•å…¥äº†transformeræ¶æ„ï¼Œèµ·åˆçš„ç›®çš„æ˜¯ä¸ºäº†è®­ç»ƒè¯­è¨€ç¿»è¯‘æ¨¡å‹ã€‚ä½†æ˜¯Open AIå›¢é˜Ÿå‘ç°transformeræ˜¯**å­—ç¬¦é¢„æµ‹**çš„å…³é”®è§£å†³æ–¹æ¡ˆ



### æ¨¡å‹æ¶æ„

![Attetion is all your needä¸­çš„æ¶æ„](image-1.png)

* Embeddingï¼ˆåµŒå…¥å‘é‡ï¼‰ï¼šå°†è¾“å…¥æ–‡å­—è½¬åŒ–æˆæ•°å­—ï½œæ–‡å­—å‘é‡åŒ–

```json
Token Embedding Look-Up Table:
               0         1         2         3         4         5         6         7         8         9   ...        54        55        56        57        58        59        60        61        62        63
0       0.625765  0.025510  0.954514  0.064349 -0.502401 -0.202555 -1.567081 -1.097956  0.235958 -0.239778  ...  0.420812  0.277596  0.778898  1.533269  1.609736 -0.403228 -0.274928  1.473840  0.068826  1.332708
1      -0.497006  0.465756 -0.257259 -1.067259  0.835319 -1.956048 -0.800265 -0.504499 -1.426664  0.905942  ...  0.008287 -0.252325 -0.657626  0.318449 -0.549586 -1.464924 -0.557690 -0.693927 -0.325247  1.243933
2       1.347121  1.690980 -0.124446 -1.682366  1.134614 -0.082384  0.289316  0.835773  0.306655 -0.747233  ...  0.543340 -0.843840 -0.687481  2.138219  0.511412  1.219090  0.097527 -0.978587 -0.432050 -1.493750
3       1.078523 -0.614952 -0.458853  0.567482  0.095883 -1.569957  0.373957 -0.142067 -1.242306 -0.961821  ... -0.882441  0.638720  1.119174 -1.907924 -0.527563  1.080655 -2.215207  0.203201 -1.115814 -1.258691
4       0.814849 -0.064297  1.423653  0.261726 -0.133177  0.211893  1.449790  3.055426 -1.783010 -0.832339  ...  0.665415  0.723436 -1.318454  0.785860 -1.150111  1.313207 -0.334949  0.149743  1.306531 -0.046524
...          ...       ...       ...       ...       ...       ...       ...       ...       ...       ...  ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...
100064 -0.898191 -1.906910 -0.906910  1.838532  2.121814 -1.654444  0.082778  0.064536  0.345121  0.262247  ...  0.438956  0.163314  0.491996  1.721039 -0.124316  1.228242  0.368963  1.058280  0.406413 -0.326223
100065  1.354992 -1.203096 -2.184551 -1.745679 -0.005853 -0.860506  1.010784  0.355051 -1.489120 -1.936192  ...  1.354665 -1.338872 -0.263905  0.284906  0.202743 -0.487176 -0.421959  0.490739 -1.056457  2.636806
100066 -0.436116  0.450023 -1.381522  0.625508  0.415576  0.628877 -0.595811 -1.074244 -1.512645 -2.027422  ...  0.436522  0.068974  1.305852  0.005790 -0.583766 -0.797004  0.144952 -0.279772  1.522029 -0.629672
100067  0.147102  0.578953 -0.668165 -0.011443  0.236621  0.348374 -0.706088  1.368070 -1.428709 -0.620189  ...  1.130942 -0.739860 -1.546209 -1.475937 -0.145684 -1.744829  0.637790 -1.064455  1.290440 -1.110520
100068  0.415268 -0.345575  0.441546 -0.579085  1.110969 -1.303691  0.143943 -0.714082 -1.426512  1.646982  ... -2.502535  1.409418  0.159812 -0.911323  0.856282 -0.404213 -0.012741  1.333426  0.372255  0.722526

[100,069 rows x 64 columns]
```

è¿™é‡Œæ¯ä¸€ä¸ªè¡Œä»£è¡¨ä¸€ä¸ªTokenï¼Œè€Œæ¯ä¸ªç»´åº¦è¡¨ç¤ºä¸€ç§åœºæ™¯ï¼ˆæ¯”å¦‚è¿™ä¸ªè¯çš„å½¢å®¹è¯ã€åŠ¨è¯ã€åæ¬¡etcï¼‰

* Positional-Encodingï¼ˆä½ç½®ä¿¡æ¯-åµŒå…¥å‘é‡ï¼‰ï¼šè¾“å…¥æ–‡å­—ç›¸å¯¹ä½ç½®å…³ç³» ï½œ å‘é‡æ·»åŠ ä½ç½®ä¿¡æ¯

ä½ç½®çŸ©é˜µåº”è¯¥æ˜¯ä¸€ä¸ªå’Œè¾“å…¥çŸ©é˜µæƒ³åŒç»´åº¦çš„çŸ©é˜µï¼Œåé¢éœ€è¦ç›¸åŠ 

```json
Position Embedding Look-Up Table:
          0         1         2         3         4         5         6         7         8         9   ...        54        55        56        57        58        59        60        61        62        63
0   0.000000  1.000000  0.000000  1.000000  0.000000  1.000000  0.000000  1.000000  0.000000  1.000000  ...  0.000000  1.000000  0.000000  1.000000  0.000000  1.000000  0.000000  1.000000  0.000000  1.000000
1   0.841471  0.540302  0.681561  0.731761  0.533168  0.846009  0.409309  0.912396  0.310984  0.950415  ...  0.000422  1.000000  0.000316  1.000000  0.000237  1.000000  0.000178  1.000000  0.000133  1.000000
2   0.909297 -0.416147  0.997480  0.070948  0.902131  0.431463  0.746904  0.664932  0.591127  0.806578  ...  0.000843  1.000000  0.000632  1.000000  0.000474  1.000000  0.000356  1.000000  0.000267  1.000000
3   0.141120 -0.989992  0.778273 -0.627927  0.993253 -0.115966  0.953635  0.300967  0.812649  0.582754  ...  0.001265  0.999999  0.000949  1.000000  0.000711  1.000000  0.000533  1.000000  0.000400  1.000000
4  -0.756802 -0.653644  0.141539 -0.989933  0.778472 -0.627680  0.993281 -0.115730  0.953581  0.301137  ...  0.001687  0.999999  0.001265  0.999999  0.000949  1.000000  0.000711  1.000000  0.000533  1.000000
5  -0.958924  0.283662 -0.571127 -0.820862  0.323935 -0.946079  0.858896 -0.512150  0.999947 -0.010342  ...  0.002108  0.999998  0.001581  0.999999  0.001186  0.999999  0.000889  1.000000  0.000667  1.000000
6  -0.279415  0.960170 -0.977396 -0.211416 -0.230368 -0.973104  0.574026 -0.818837  0.947148 -0.320796  ...  0.002530  0.999997  0.001897  0.999998  0.001423  0.999999  0.001067  0.999999  0.000800  1.000000
7   0.656987  0.753902 -0.859313  0.511449 -0.713721 -0.700430  0.188581 -0.982058  0.800422 -0.599437  ...  0.002952  0.999996  0.002214  0.999998  0.001660  0.999999  0.001245  0.999999  0.000933  1.000000
8   0.989358 -0.145500 -0.280228  0.959933 -0.977262 -0.212036 -0.229904 -0.973213  0.574318 -0.818632  ...  0.003374  0.999994  0.002530  0.999997  0.001897  0.999998  0.001423  0.999999  0.001067  0.999999
9   0.412118 -0.911130  0.449194  0.893434 -0.939824  0.341660 -0.608108 -0.793854  0.291259 -0.956644  ...  0.003795  0.999993  0.002846  0.999996  0.002134  0.999998  0.001600  0.999999  0.001200  0.999999
10 -0.544021 -0.839072  0.937633  0.347628 -0.612937  0.790132 -0.879767 -0.475405 -0.020684 -0.999786  ...  0.004217  0.999991  0.003162  0.999995  0.002371  0.999997  0.001778  0.999998  0.001334  0.999999
11 -0.999990  0.004426  0.923052 -0.384674 -0.097276  0.995257 -0.997283 -0.073661 -0.330575 -0.943780  ...  0.004639  0.999989  0.003478  0.999994  0.002609  0.999997  0.001956  0.999998  0.001467  0.999999
12 -0.536573  0.843854  0.413275 -0.910606  0.448343  0.893862 -0.940067  0.340989 -0.607683 -0.794179  ...  0.005060  0.999987  0.003795  0.999993  0.002846  0.999996  0.002134  0.999998  0.001600  0.999999
13  0.420167  0.907447 -0.318216 -0.948018  0.855881  0.517173 -0.718144  0.695895 -0.824528 -0.565821  ...  0.005482  0.999985  0.004111  0.999992  0.003083  0.999995  0.002312  0.999997  0.001734  0.999998
14  0.990607  0.136737 -0.878990 -0.476839  0.999823 -0.018796 -0.370395  0.928874 -0.959605 -0.281349  ...  0.005904  0.999983  0.004427  0.999990  0.003320  0.999995  0.002490  0.999997  0.001867  0.999998
15  0.650288 -0.759688 -0.968206  0.250154  0.835838 -0.548975  0.042249  0.999107 -0.999519  0.031022  ...  0.006325  0.999980  0.004743  0.999989  0.003557  0.999994  0.002667  0.999996  0.002000  0.999998

[16 rows x 64 columns]
```

* Multi-Head Attentionï¼šå¾—åˆ°æ¯ä¸ªè¯ç›¸å¯¹äºå…¶ä»–è¯çš„ä¸€ä¸ªæ¦‚ç‡è¡¨

![çŸ©é˜µä¹˜æ³•](image.png)



![Transformeræ¶æ„ä¸­çš„Multi-Head Attentionæœºåˆ¶](image-1.png)

QçŸ©é˜µå’ŒKçŸ©é˜µç›¸ä¹˜ï¼Œè¡¨ç¤ºæ¯ä¸ªå­—ç¬¦ç›¸å¯¹äºå…¶ä»–å­—ç¬¦çš„ç›¸ä¼¼åº¦ï¼Œå¾—åˆ°çš„ç»“æœçŸ©é˜µä¸­ï¼Œå€¼è¶Šå¤§ç›¸ä¼¼åº¦è¶Šé«˜

* Normalization Layerï¼šå°†æ¦‚ç‡åˆ†å¸ƒæ•´ä½“å‘ä¸‹å¹³ç§»ï¼Œå°çš„å€¼è¾ƒå°ç§»åŠ¨ï¼Œå¤§çš„å€¼è¾ƒå¤§ç§»åŠ¨ã€‚ä¹Ÿå«å½’ä¸€åŒ–

è²Œä¼¼å’ŒSoftMaxæ˜¯ä¸€æ ·çš„ï¼Œä¹Ÿæ˜¯æŠŠå€¼è¿›è¡Œä¸€æ¬¡åˆç†æ¢³ç†

* Linearï¼šç»“åˆè¯æ±‡è¡¨ï¼Œå½¢æˆä¸€ä¸ªéå¸¸å¤§çš„çŸ©é˜µï¼ˆè¾“å…¥ä¸­åäººæ°‘å…±å’Œå›½ï¼Œå‡è®¾è¯æ±‡è¡¨ä¸º10000ï¼ŒçŸ©é˜µä¸º7\*10000ï¼‰å¯ä»¥æ‰¾å‡ºæ¦‚ç‡æœ€å¤§çš„å­—ç¬¦

  å‰ä¸€æ­¥è¾“å‡ºçš„16\*64çŸ©é˜µå’Œ100000è¯æ±‡è¡¨çŸ©é˜µç›¸ä¹˜ï¼Œå¾—åˆ°çš„ç»“æœå°±æ˜¯æ¯ä¸ªtokenå’Œè¯æ±‡è¡¨å€¼çš„æ¦‚ç‡å€¼ã€‚æ¯ä¸ªtokenå¯¹åº”çš„æœ€å¤§å€¼å°±æ˜¯æœ€æœ‰å¯èƒ½è¢«é¢„æµ‹å‡ºç°çš„å•è¯

### Pre-Training é¢„è®­ç»ƒ

é¢„è®­ç»ƒä¹‹åå®é™…åªèƒ½è¾¾åˆ°æ–‡æœ¬å®Œæˆå™¨compeleterï¼Œé€šè¿‡Fine-Tuningå’ŒRLHFæ‰èƒ½åšåˆ°ç”Ÿæˆå™¨generator

### Fine-tuning å¾®è°ƒ

### RLHF å¼ºåŒ–å­¦ä¹ 

Reinforce Learning from Human Feedback&#x20;

æ¨¡å‹ç»™å‡ºçš„å¤šä¸ªç»“æœåé¦ˆç»™ç”¨æˆ·ï¼Œç”¨æˆ·é€šè¿‡é€‰æ‹©æœ€åˆé€‚çš„ä¸€é¡¹æ¥ä¿®æ”¹æ¨¡å‹å‚æ•°



### Prompt Engineering

## LLM-ç¼–ç è¿‡ç¨‹ğŸ§‘â€ğŸ’»

### 1. æ„é€ è¾“å…¥å¼ é‡

ä¸‹è½½æ•°æ®é›†åˆ°æœ¬åœ°(å¯é€‰)

#### ä¸‹è½½å¹¶å¯¼å…¥å¿…è¦åŒ…

```python
import torch    // æ‰‹åŠ¨å®‰è£…
import torch.nn as nn
import torch.nn.Functional as F
import os       // å†…ç½®æ¨¡å—
import request  // æ‰‹åŠ¨å®‰è£…  2.32.4
```

#### è·å–æ•°æ®é›†

```python
# å¦‚æœæ²¡æœ‰æ–‡ä»¶ è¯·æ±‚æ•°æ®é›†
if not os.path.exists("sales-textbooks.txt"):
    url = "https://huggingface.co/datasets/goendalf666/sales-textbook_for_convincing_and_selling/resolve/main/sales_textbook.txt?download=true"
    with open("sales-textbooks.txt","wb") as f:
        f.write(requests.get(url).content)

with open("sales-textbooks.txt","r") as f:
    text = f.read()
    print("ğŸš€ text:",text)
```

1. TokenizeåŒ–

å‡†å¤‡å¥½æ•°æ®é›†ä¹‹åï¼Œéœ€è¦å°†æ‹¿åˆ°çš„æ•°æ®é›†è¿›è¡ŒTokenizeåŒ–

Tokenizeçš„æ–¹æ³•æ¯”è¾ƒå¤šï¼š

#### å®šä¹‰è¶…å‚æ•°

```python
# hyperparameter
context_length = 16 
d_modal = 64
```

context\_lengthæ˜¯è®­ç»ƒå¥å­çš„tokené•¿åº¦

#### è®­ç»ƒé›†ä¸­æå–æ–‡æœ¬

```python
data = train_data
idxs = torch.randint(low=0,high=len(data) - context_length,size=(batch_size,))
x_batch = torch.stack([data[idx:idx + context_length] for idx in idxs])
y_batch = torch.stack([data[idx+1:idx + context_length+1] for idx in idxs])
```

#### åˆ›å»ºInputEmbeddingTable

ç»™æ¯ä¸ªtokenæ‹“å±•ç»´åº¦ã€‚æ¯ä¸ªç»´åº¦ä»£è¡¨è¿™ä¸ªtokençš„ä¸€ç§å¯èƒ½æ€§

åˆå§‹åŒ–æ—¶ï¼Œæ¯ä¸ªtokenå¯¹åº”å¯èƒ½æ€§çš„åˆå§‹å€¼æ˜¯éšæœºå€¼ï¼Œéšç€è®­ç»ƒè¿™äº›éšæœºå€¼å˜å¾—å‡†ç¡®ä¸å†å”¯ä¸€

æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ä½¿ç”¨64ç»´ï¼Œè¡¨ç¤ºæ¯ä¸ªtokenæœ‰ç€64ç§å¯èƒ½æ€§

åˆ›å»ºä¹‹åçš„å¼ é‡å½¢æ€åº”è¯¥æ˜¯4âœ–ï¸16âœ–ï¸64

```python
max_token_value = tokenized_text.max().item()
input_embedding_lookup_table = nn.Embedding(max_token_value+1,d_modal)
x_batch_embedding = input_embedding_lookup_table(x_batch)
y_batch_embedding = input_embedding_lookup_table(y_batch) # y_batch_embedding.shape = (4 * 16 * 64)
```

#### åˆ›å»ºpositional encoding

å…ˆæ„é€ ä¸€ä¸ªå…¨0çŸ©é˜µï¼Œä¹‹åçŸ©é˜µå—é‡Œçš„å€¼æŒ‰ç…§ä¸‹é¢å…¬ç¤ºæ„é€ 

> PE(pos, 2i) = sin(pos / 10000^(2i/d\_model))
>
> PE(pos, 2i+1) = cos(pos / 10000^(2i/d\_model))

```python
import math
positinal_encoding_lookup_table = torch.zeros(context_length,d_modal)
position = torch.arange(0,context_length,dtype=torch.float).unsqueeze(1)
div_item = torch.exp(torch.arange(0,d_modal,2).float() * (-math.log(10000.0) / d_modal))

positinal_encoding_lookup_table[:,0::2] = torch.sin(position * div_item)
positinal_encoding_lookup_table[:,1::2] = torch.cos(position * div_item)
positinal_encoding_lookup_table = positinal_encoding_lookup_table.unsqueeze(0).expand(batch_size,-1,-1)

x = x_batch_embedding + positinal_encoding_lookup_table
y = y_batch_embedding + positinal_encoding_lookup_table
```

![ç­‰å¾…æœºå™¨å­¦ä¹ æ›´æ–°æƒé‡å€¼çš„çŸ©é˜µ](image-2.png)

### 2. Transformeræ¶æ„ç¼–å†™

#### Get Qã€Kã€V

```python
Wq = nn.Linear(d_modal,d_modal)
Wk = nn.Linear(d_modal,d_modal)
Wv = nn.Linear(d_modal,d_modal)

Q = Wq(x)
K = Wk(x)
V = Wv(x)

```

#### Calculate QKV

![](image-3.png)

```python
mask = torch.triu(torch.ones(context_length,context_length),diagonal=1).bool()
output = output.masked_fill(mask,float('-inf'))
print(pd.DataFrame(output[0,0].detach().numpy()))
```



![](image-4.png)

#### æ³¨æ„åŠ›æœºåˆ¶ä¸­æ·»åŠ mask

```python
mask = torch.triu(torch.ones(context_length,context_length),diagonal=1).bool()
output = output.masked_fill(mask,float('-inf'))
# print(pd.DataFrame(output[0,0].detach().numpy()))
```

#### softmax

```python
attentionn_score = F.softmax(output,dim=1)
```

#### attention @ V

```python
## attention @ V
A = attentionn_score @ V
```

#### concateate

```python
A = A.transpose(1,2).reshape(batch_size,-1,d_modal)
Wo = nn.Linear(d_modal,d_modal)
output = Wo(A)
```

#### residual connection æ®‹å·®è¿æ¥

```python
output = output + layer_norm_output
```

#### LayerNorm

> å–å‡ºå¼ é‡ä¸­çš„æŸä¸€ä¸ªTokenè¡Œï¼Œå‡å€¼é è¿‘0ï¼Œæ–¹å·®æ¥è¿‘1
>
> å‡å€¼ï¼š$$\mu = \frac{1}{H} \sum_{i=1}^{H} x_i$$
>
> æ–¹å·®ï¼š$$\sigma^2 = \frac{1}{H} \sum_{i=1}^{H} (x_i - \mu)^2$$

å½’ä¸€åŒ–ï¼š



$$\hat{x}_i = \frac{x_i - \mu}{\sqrt{\sigma^2 + \epsilon}}$$

$$\epsilon
$$æ˜¯ä¸€ä¸ªæ— é™å°çš„æ•°ï¼Œé˜²æ­¢é™¤0

![](image-5.png)

é‡æ–°ç¼©æ”¾å’Œåç§»ï¼š

$$Y_i = \gamma \hat{x}_i + \beta$$

```python
output = layer_norm(output)
```

#### FeedForward

```python
output = nn.Linear(d_modal,d_modal * 4)(layer_norm_output) # æ”¾å¤§ç»´åº¦
output = nn.ReLU()(output)                      # æ¿€æ´»å‡½æ•°å¤„ç†
output = nn.Linear(d_modal * 4,d_modal)(output)
```

#### LinearLayer

```python
output = nn.Linear(d_modal,max_token_value+1)(output)
```

![æ¶æ„è¾“å‡º](image-6.png)

#### Decoding Strategy(Inferenceè¿‡ç¨‹)

æ¨ç†ç­–ç•¥ï¼š

1. Greedy Search

   æ°¸è¿œè¿”å›logitsæœ€é«˜çš„token

   å¾ˆå¤šæ¨¡å‹ä»¥åŠPytorch**é»˜è®¤ç­–ç•¥**

   ![](image-7.png)

2. Beam Search

åŒæ—¶é€‰å–logitsæœ€å¤§çš„Top Nä¸ªBeamï¼Œç­‰å¾…æ‰€æœ‰Tokené¢„æµ‹ç»“æŸåï¼Œè¿”å›æ‰€æœ‰Token logitæ€»å’Œæœ€å¤§çš„ä¸€ä¸ªBeam

ç¼ºç‚¹ï¼šç­‰å¾…æ—¶é—´ä¹…ï¼›é¢å‘Cç«¯åœºæ™¯ä¸é€‚ç”¨ï¼Œç ”ç©¶ä½¿ç”¨æ›´åˆé€‚

![Beam Search]()

* TOP- K Sampling

  &#x20;**é€‰å–**éšæœºKä¸ªæœ€é«˜æ¦‚ç‡å–æ ·

* TOP- P Sampling

**é€‰å–**è¶…è¿‡é˜ˆå€¼

* Temperature

GreadySearch + TOP-P\[Optional] + Temperatureæ˜¯ç›®å‰æœ€ä¸»æµçš„ç­–ç•¥ç»„åˆ

![temperatureå¯¹probabilityçš„å½±å“]()

![modelå‚æ•°è®¾ç½®]()

### GPT3.5ï¼ˆè¡¥å……çŸ¥è¯†ï¼‰

transformer\_blockï¼š12

### 3. ç”¨æ¨¡å‹è®­ç»ƒå°è¯´é¡¹ç›®

A. è®­ç»ƒæ¨¡å‹

```python
âš¡ ~/LLM-Novel python3 train-model.py
æ•°æ®é›†åˆè®¡æœ‰ 16,397,115 tokens
Step: 0 Training Loss: 11.719 Validation Loss: 11.705
Step: 20 Training Loss: 6.159 Validation Loss: 6.228
Step: 40 Training Loss: 6.086 Validation Loss: 6.108
Step: 60 Training Loss: 6.002 Validation Loss: 6.034
Step: 80 Training Loss: 5.517 Validation Loss: 5.569
Step: 100 Training Loss: 5.1 Validation Loss: 5.135
Step: 120 Training Loss: 4.899 Validation Loss: 4.962
```

B. æ‰“å°æ¨¡å‹å‚æ•°

```python
âš¡ ~/LLM-Novel python3 show-parameter.py 
æ¨¡å‹å‚æ•°é‡ä¸º:140573600
```

C. æ¨æ–­

```python
âš¡ ~/LLM-Novel python3 Inference.py 
---------------
Liu Feng was born inï¼Œå°±åƒè¿™ä¸–ç•Œä¸­çš„ä¸€ä¸ªæ’æ˜Ÿçƒä¸€æ ·å­˜åœ¨ã€‚è¦çŸ¥é“ï¼Œåˆ°ç›®å‰ä¸ºæ­¢ï¼Œä¸ä¼šæœ‰å…¶å®ƒä½“ç³»ï¼Œï¼Œåˆæ€ä¹ˆèƒ½åœ¨é‚£åœºåˆå‘¢ï¼Ÿæ­£æ˜¯ä¸ºäº†ç ”ç©¶å®ƒçš„ç»„ç»‡ ï¼Œå¯¹äºç²¾åŠ›æ„æˆçƒ­ä½“ç§¯ç±»åŠä¸ªç‰©ç§æ–°å½¢çš„èƒ½åŠ›ã€‚å®ƒä¸€ç›´ä»¥ä¸ºä¼šç”¨ä»€ä¹ˆæ ·å­ï¼Œä½†æ˜¯å´æ²¡èƒ½ç”¨äºå­è¿è½¬ã€‚å®ƒæ¯ä¸ªæŠ€æœ¯ä¸Šéƒ½èƒ½å‘ç”Ÿæ•°ç™¾ä¸‡å¹´æ—¶é—´ã€‚åœ¨æ—¶é—´ç™¾ä¸‡å¯Œï¿½é‡Œï¼Œäººç±»å¹¶ä¸å­˜åœ¨æ ä¸Šæ˜Ÿé™…ä¹‹ç±»çš„æ˜Ÿç©ºç‰©ï¼Œå› ä¸ºä¸èƒ½å¼•äº§ç”Ÿæ°”å€™çš„å·¨å¤§é›¶æ— æ¯”ï¼Œä½†å®ƒä»¬çš„çµé­‚æ€»ä¼šå¤±å»æ— æ³•æ€è€ƒçš„å…³ç³»ã€‚
åœ¨åœ°çƒä½œåˆ†å…¬å…±å®ƒä»¬ä¹‹é—´çš„è¡¨é¢ï¼Œæ¯ä¸€æ¶â€œé»‘æ¡¶å…ƒè€å¸½â€ç°åœ¨ä¸€ç¿»è¿‡å¤§åœ°ï¼Œä¾¿ä¼šåœä¸‹æ¥é˜¿ç€èœ‚çŠæ˜Ÿçƒä¸°è¡Œå‰çº¿çš„å°å‹å¼•åŠ›å°ï¼Œå¹¶ä¸”è«é•¿æ— æ¯”ï¼Œè¿™æ˜¯æ²¡æœ‰é›¨åŒ… çš„ã€å‹åŠ›æœç€ç™½æ¡¥ä¹‹å¤–çš„åœ°æ–¹é£è¡Œæ´é«˜é«˜å±‚ã€‚
å¯æ˜¯ï¼Œå¦ä¸€ä¸ªå¸¦ç€ç”Ÿç‰©ä½¿çš±çœ‰å¤´é—ªç€å…‰ï¼Œä¸œå‡ æ ¹æ‰‹æŒ‡ç€å¸½å­ï¼Œâ€œçœ‹æ¥ï¼Œè¿™æ˜¯ä½ ä»¬çš„è„šçƒé‚£ä¹ˆé«˜ï¼Œæš—ç¤ºç¤¼è²Œã€‚â€ä»–é—®é“ã€‚
â€œæ˜¯å•Šï¼å¼„è¿™çš„ï¼Ÿâ€â€”â€”ä»–æ„Ÿåˆ°çº›ä¸ä»ä¸åŠ¨æ‘‡å¤´ã€‚
ä»–ç¥äº†ç”²æ¿æŸ”è½¯çš„ç°çœ¼ï¼Œåƒç”·äººæ‰€ç¼“ç¼“ä¸å·²çš„ç…¦åœ°æ¼«æ¸¸çš„ã€‚å®ƒå°†æ„è¯†æ­»ï¼Œéšè—åœ¨ä¸æ„Ÿå…´è¶£çš„ä¸€åˆ»ã€‚ä»–æ”¾è¿‡æ‰€æœ‰è¢«è½¯ç¡¬åˆè¢‹å’Œç¯å…‰ç…§äº®ï¼Œå®ƒè‚šå­åˆšè¿‡äº†
```

D.  å¾®è°ƒï¼ˆäºŒæ¬¡è®­ç»ƒï¼‰

ä¸Šé¢ä¸‰æ­¥å®Œæˆä¹‹åï¼Œæ¨¡å‹ä»…ä»…åªèƒ½åšåˆ°Tokençš„è‡ªæˆ‘å»¶ä¼¸ã€‚æ— æ³•åšåˆ°ä¸€é—®ä¸€ç­”æ¨¡å¼ï¼Œæƒ³è¦åšåˆ°ä¸€é—®ä¸€ç­”æ¨¡å¼éœ€è¦è¿›è¡Œ**å¾®è°ƒ**è¿‡ç¨‹ã€‚ç®€å•è¯´å¯¹æ¨¡å‹ä¸æ–­çš„è¾“å…¥**å¤§é‡çš„Q\&Aæµ‹è¯•ç”¨ä¾‹**

è¿™æ­¥éª¤å®Œæˆä¹‹åï¼Œè¾“å…¥â€œè¯·å¸®æˆ‘å†™ä¸ªå°è¯´â€ï¼Œæ¨¡å‹åˆ™ä¸ä¼šè¿›è¡Œæ¨æ–­å»¶ä¼¸è€Œæ˜¯å¯ä»¥å¯ä»¥ç›´æ¥å›ç­”æˆ‘çš„é—®é¢˜

å¾®è°ƒçš„ä»£ç å’Œè®­ç»ƒçš„ä»£ç å‡ ä¹ä¸€æ ·ï¼Œåªæ˜¯ä¸€äº›è¶…å‚æ•°ä¸ä¸€æ ·

å¦å¤–ï¼Œå¾®è°ƒçš„æ•°æ®é›†æ˜¯è¦æ¯”è®­ç»ƒè¦å°çš„

æ¨¡å‹æ–°çš„å¾®è°ƒæ–¹å¼ï¼š

* compile

* lora

```python
# å¾®è°ƒæ–‡ä»¶
import os
import sys
import pickle
from contextlib import nullcontext
import torch
import tiktoken
from aim import Run
from model import Model
import json

# Hyperparameters
batch_size = 8  # How many batches per training step
context_length = 128  # Length of the token chunk each batch
max_iters = 1000  # Total of training iterations <- Change this to smaller number for testing
learning_rate = 1e-4  # 0.001
eval_interval = 10  # How often to evaluate
eval_iters = 10  # Number of iterations to average for evaluation
device = 'cuda' if torch.cuda.is_available() else 'cpu'  # Use GPU if it's available.
TORCH_SEED = 1337
torch.manual_seed(TORCH_SEED)

# å‡†å¤‡è®­ç»ƒæ•°æ®
with open('data/scifi-finetune.json', 'r') as file:
    alpaca = json.load(file)
    text = alpaca[1000:5001]

# print(text)
# sys.exit(0)

# Using TikToken (Same as GPT3) to tokenize the source text
encoding = tiktoken.get_encoding("cl100k_base")
tokenized_text = encoding.encode(str(text))
tokenized_text = torch.tensor(tokenized_text, dtype=torch.long, device=device)  # å°†77,919ä¸ªtokens è½¬æ¢åˆ°Pytorchå¼ é‡ä¸­

total_tokens = encoding.encode_ordinary(str(text))
print(f"æ•°æ®é›†åˆè®¡æœ‰ {len(total_tokens):,} tokens")

# Split train and validation
train_size = int(len(tokenized_text) * 0.9)
train_data = tokenized_text[:train_size]
val_data = tokenized_text[train_size:]

# Initialize the model
model = Model()
model.load_state_dict(torch.load('model/model-scifi.pt'))
model.to(device)

# get batch
def get_batch(split: str):
    data = train_data if split == 'train' else val_data
    idxs = torch.randint(low=0, high=len(data) - context_length, size=(batch_size,))
    x = torch.stack([data[idx:idx + context_length] for idx in idxs]).to(device)
    y = torch.stack([data[idx + 1:idx + context_length + 1] for idx in idxs]).to(device)
    return x, y

# calculate the loss
@torch.no_grad()
def estimate_loss():
    out = {}
    model.eval()
    for split in ['train', 'valid']:
        losses = torch.zeros(eval_iters)
        for k in range(eval_iters):
            x_batch, y_batch = get_batch(split)
            logits, loss = model(x_batch, y_batch)
            losses[k] = loss.item()
        out[split] = losses.mean()
    model.train()
    return out

# Create the optimizer
optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)
tracked_losses = list()
for step in range(max_iters):
    if step % eval_iters == 0 or step == max_iters - 1:
        losses = estimate_loss()
        tracked_losses.append(losses)
        print('Step:', step, 'Training Loss:', round(losses['train'].item(), 3), 'Validation Loss:', round(losses['valid'].item(), 3))

    xb, yb = get_batch('train')
    logits, loss = model(xb, yb)
    optimizer.zero_grad(set_to_none=True)
    loss.backward()
    optimizer.step()

# Save the model
torch.save(model.state_dict(), 'model/model-scifi-finetune.pt')
```

### 4. Stable Diffusionç”Ÿæˆå›¾ç‰‡

ä¹Ÿå«æ–‡ç”Ÿå›¾ç‰‡æŠ€æœ¯ã€‚æ˜¯Stability.aiå…¬å¸åŸºäºTransformeræ¶æ„é¢„è®­ç»ƒçš„å¤§æ¨¡å‹

#### æœ€æ–°æ¨¡å‹ç®€ä»‹

#### ç¬¬ä¸‰æ–¹æ¨¡å‹ä½¿ç”¨æ–¹å¼

```python
# æœ¬é¡¹ç›®ä½¿ç”¨æ¥è‡ªHuggingFaceå·²é¢„è®­ç»ƒå¥½çš„æ¨¡å‹

from diffusers.pipelines.auto_pipeline import AutoPipelineForText2Image
from PIL import Image
import time
import torch

start_time = time.time()
pipe = AutoPipelineForText2Image.from_pretrained("stabilityai/sdxl-turbo", torch_dtype=torch.float16, variant="fp16")
pipe.to("cuda")

prompt = "ç”»ä¸€ä¸ªå¤§æµ·"
image = pipe(prompt=prompt, num_inference_steps=1, guidance_scale=0.0).images[0]
image.save('test2.png')

end_time = time.time()

gap_time = end_time - start_time
print("script execution time:",gap_time," seconds")
```

#### æ¨¡å‹è¯„ä»·æŒ‡æ ‡ & Scalling Law

N - æ¨¡å‹å‚æ•°é‡ï¼ˆWeight and Biasï¼‰

D - è®­ç»ƒæ•°æ®é‡

![è®¡ç®—Losså…¬å¼ Deepmindç‰ˆ]()

> Chinchilla Scalling Law:
>
> 1. **æ•¸æ“šé‡ï¼ˆTokensæ•¸ï¼‰æ‡‰è©²è¦ç´„ç­‰æ–¼æ¨¡å‹åƒæ•¸é‡çš„20å€**
>
> 2. **ä¸¦ä¸”æ•¸æ“šé‡è·Ÿæ¨¡å‹åƒæ•¸é‡è¦åŒæ¯”æ”¾å¤§**ï¼ˆEx: æ¨¡å‹æ”¾å¤§ä¸€å€ï¼Œæ•¸æ“šä¹Ÿè¦è·Ÿè‘—å¢åŠ ä¸€å€ï¼‰
>
> 3. [Chinchilla data-optimal scaling laws: In plain English: In plain English](https://lifearchitect.ai/chinchilla/)

![è®¡ç®—é‡å’Œæ¨¡å‹å°ºå¯¸å…³ç³»]()

![DeepMind]()



![Scalling Lawè§„å¾‹]()

![]()



## LLM-ç†è®ºè¿›é˜¶ğŸ¥³

### Linear Tranformation

[ å¤§æ¨¡å‹- LLM](https://pcnqdohorvbp.feishu.cn/wiki/Ma0qw4dEriEcXjkhh9BchYtanFh#share-OMkndoRAFok6jExzZJLcHD4MnAb)

![Wq(X)]()

ä¸­é—´çŸ©é˜µï¼Œ$$512 \times 2$$ä¸­çš„2ä»£è¡¨`context-length`,æ¯”å¦‚â€œè‹¹æœâ€äºŒå­—

