<!doctype html><html lang=en dir=ltr><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>LLM-å¤§æ¨¡å‹ | Liu Feng</title><meta name=generator content="Hugo Eureka 0.9.3"><link rel=stylesheet href=https://yaoyuanArtemis.github.io/css/eureka.min.9cec6350e37e534b0338fa9a085bf06855de3b0f2dcf857e792e5e97b07ea905d4d5513db554cbc26a9c3da622bae92d.css><script defer src=https://yaoyuanArtemis.github.io/js/eureka.min.fa9a6bf6d7a50bb635b4cca7d2ba5cf3dfb095ae3798773f1328f7950028b48c17d06276594e1b5f244a25a6c969a705.js></script>
<link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload href="https://fonts.googleapis.com/css2?family=Lora:wght@400;600;700&family=Noto+Serif+SC:wght@400;600;700&display=swap" as=style onload='this.onload=null,this.rel="stylesheet"'><link rel=stylesheet href=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.4.0/build/styles/base16/solarized-light.min.css media=print onload='this.media="all",this.onload=null' crossorigin><script defer src=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.4.0/build/highlight.min.js crossorigin></script>
<script defer src=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.4.0/build/languages/dart.min.js crossorigin></script>
<link rel=stylesheet href=https://yaoyuanArtemis.github.io/css/highlightjs.min.2958991528e43eb6fc9b8c4f2b8e052f79c4010718e1d1e888a777620e9ee63021c2c57ec7417a3108019bb8c41943e6.css media=print onload='this.media="all",this.onload=null'><script defer type=text/javascript src=https://yaoyuanArtemis.github.io/js/fontawesome.min.43188b2fd13c27dbb34eb32b5138ddb548a5804f430376f50f0ee994d02357ea9c4f3f1380a0d928b702dc2004164443.js></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css integrity=sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ media=print onload='this.media="all",this.onload=null' crossorigin><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js integrity=sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY crossorigin></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})})</script><script defer src=https://cdn.jsdelivr.net/npm/mermaid@8.14.0/dist/mermaid.min.js integrity=sha384-atOyb0FxAgN9LyAc6PEf9BjgwLISyansgdH8/VXQH8p2o5vfrRgmGIJ2Sg22L0A0 crossorigin></script>
<link rel=icon type=image/png sizes=32x32 href=https://yaoyuanArtemis.github.io/images/favicon_hu544df93bd7abf6c30dbc8fc32cd03fe4_32399_32x32_fill_box_center_3.png><link rel=apple-touch-icon sizes=180x180 href=https://yaoyuanArtemis.github.io/images/favicon_hu544df93bd7abf6c30dbc8fc32cd03fe4_32399_180x180_fill_box_center_3.png><meta name=description content="LLMåŸºç¡€ç¯‡æ€»ç»“"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://yaoyuanArtemis.github.io/posts/"},{"@type":"ListItem","position":2,"name":"LLM-å¤§æ¨¡å‹","item":"https://yaoyuanArtemis.github.io/posts/llm/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://yaoyuanArtemis.github.io/posts/llm/"},"headline":"LLM-å¤§æ¨¡å‹ | Liu Feng","datePublished":"2021-03-31T13:11:22+08:00","dateModified":"2021-03-31T13:11:22+08:00","wordCount":1535,"author":{"@type":"Person","name":["example-author"]},"publisher":{"@type":"Person","name":"Liu Feng","logo":{"@type":"ImageObject","url":"https://yaoyuanArtemis.github.io/images/favicon.png"}},"description":"LLMåŸºç¡€ç¯‡æ€»ç»“"}</script><meta property="og:title" content="LLM-å¤§æ¨¡å‹ | Liu Feng"><meta property="og:type" content="article"><meta property="og:image" content="https://yaoyuanArtemis.github.io/images/favicon.png"><meta property="og:url" content="https://yaoyuanArtemis.github.io/posts/llm/"><meta property="og:description" content="LLMåŸºç¡€ç¯‡æ€»ç»“"><meta property="og:locale" content="en"><meta property="og:site_name" content="Liu Feng"><meta property="article:published_time" content="2021-03-31T13:11:22+08:00"><meta property="article:modified_time" content="2021-03-31T13:11:22+08:00"><meta property="article:section" content="posts"><meta property="article:tag" content="LLM"><meta property="article:tag" content="äººå·¥æ™ºèƒ½"><body class="flex min-h-screen flex-col"><header class="min-h-16 pl-scrollbar bg-secondary-bg fixed z-50 flex w-full items-center shadow-sm"><div class="mx-auto w-full max-w-screen-xl"><script>let storageColorScheme=localStorage.getItem("lightDarkMode");((storageColorScheme=="Auto"||storageColorScheme==null)&&window.matchMedia("(prefers-color-scheme: dark)").matches||storageColorScheme=="Dark")&&document.getElementsByTagName("html")[0].classList.add("dark")</script><nav class="flex items-center justify-between flex-wrap px-4 py-4 md:py-0"><a href=/ class="me-6 text-primary-text text-xl font-bold">Liu Feng</a>
<button id=navbar-btn class="md:hidden flex items-center px-3 py-2" aria-label="Open Navbar">
<i class="fas fa-bars"></i></button><div id=target class="hidden block md:flex md:grow md:justify-between md:items-center w-full md:w-auto text-primary-text z-20"><div class="md:flex md:h-16 text-sm md:grow pb-4 md:pb-0 border-b md:border-b-0"><a href=/details/ class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2 border-transparent me-4">Details</a>
<a href=/posts/ class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2 selected-menu-item me-4">Posts</a>
<a href=/projects/ class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2 border-transparent me-4">Projects</a></div><div class=flex><div class="relative pt-4 md:pt-0"><div class="cursor-pointer hover:text-eureka" id=lightDarkMode><i class="fas fa-adjust"></i></div><div class="fixed hidden inset-0 opacity-0 h-full w-full cursor-default z-30" id=is-open></div><div class="absolute flex flex-col start-0 md:start-auto end-auto md:end-0 hidden bg-secondary-bg w-48 rounded py-2 border border-tertiary-bg cursor-pointer z-40" id=lightDarkOptions><span class="px-4 py-1 hover:text-eureka" name=Light>Light</span>
<span class="px-4 py-1 hover:text-eureka" name=Dark>Dark</span>
<span class="px-4 py-1 hover:text-eureka" name=Auto>Auto</span></div></div></div></div><div class="fixed hidden inset-0 opacity-0 h-full w-full cursor-default z-0" id=is-open-mobile></div></nav><script>let element=document.getElementById("lightDarkMode");storageColorScheme==null||storageColorScheme=="Auto"?document.addEventListener("DOMContentLoaded",()=>{window.matchMedia("(prefers-color-scheme: dark)").addEventListener("change",switchDarkMode)}):storageColorScheme=="Light"?(element.firstElementChild.classList.remove("fa-adjust"),element.firstElementChild.setAttribute("data-icon","sun"),element.firstElementChild.classList.add("fa-sun")):storageColorScheme=="Dark"&&(element.firstElementChild.classList.remove("fa-adjust"),element.firstElementChild.setAttribute("data-icon","moon"),element.firstElementChild.classList.add("fa-moon")),document.addEventListener("DOMContentLoaded",()=>{getcolorscheme(),switchBurger()})</script></div></header><main class="grow pt-16"><div class=pl-scrollbar><div class="mx-auto w-full max-w-screen-xl lg:px-4 xl:px-8"><div class="grid grid-cols-2 gap-4 lg:grid-cols-8 lg:pt-12"><div class="bg-secondary-bg col-span-2 rounded px-6 py-8 lg:col-span-6"><article class=prose><h1 class=mb-4>LLM-å¤§æ¨¡å‹</h1><div class="text-tertiary-text not-prose mt-2 flex flex-row flex-wrap items-center"><div class="me-6 my-2"><i class="fas fa-calendar me-1"></i>
<span>2021-03-31</span></div><div class="me-6 my-2"><i class="fas fa-clock me-1"></i>
<span>8 min read</span></div><div class="me-6 my-2"><i class="fas fa-folder me-1"></i>
<a href=https://yaoyuanArtemis.github.io/categories/llm/ class=hover:text-eureka>LLM</a>
<span>,</span>
<a href=https://yaoyuanArtemis.github.io/categories/gpt/ class=hover:text-eureka>GPT</a>
<span>,</span>
<a href=https://yaoyuanArtemis.github.io/categories/machine-learning/ class=hover:text-eureka>Machine Learning</a></div></div><p>ä¸€è¨€ä»¥è”½ä¹‹ï¼Œä¸€ä¸ªLLMæ¨¡å‹å°±æ˜¯ä¸€ä¸ªæ¦‚ç‡æ•°æ®åº“ã€‚å®ƒä¸ºä»»ä½•ç»™å®šå­—ç¬¦çš„ä¸Šä¸‹æ–‡å­—ç¬¦åˆ†é…ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒ</p><h2 id=llm-åŸç†>LLM-åŸç†ğŸ˜ˆ</h2><h3 id=èƒŒæ™¯>èƒŒæ™¯</h3><p>åœ¨LLMå‡ºç°ä¹‹å‰ï¼Œæœºå™¨å¯¹ç¥ç»ç½‘ç»œçš„è®­ç»ƒå—é™äºç›¸å¯¹è¾ƒå°çš„æ•°æ®é›†ï¼Œå¯¹ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›éå¸¸æœ‰é™</p><p>Google Brainå›¢é˜Ÿåœ¨2017å¹´å‘å¸ƒäº†ã€ŠAttetion is all your needã€‹åå¼•å…¥äº†transformeræ¶æ„ï¼Œèµ·åˆçš„ç›®çš„æ˜¯ä¸ºäº†è®­ç»ƒè¯­è¨€ç¿»è¯‘æ¨¡å‹ã€‚ä½†æ˜¯Open AIå›¢é˜Ÿå‘ç°transformeræ˜¯<strong>å­—ç¬¦é¢„æµ‹</strong>çš„å…³é”®è§£å†³æ–¹æ¡ˆ</p><h3 id=æ¨¡å‹æ¶æ„>æ¨¡å‹æ¶æ„</h3><p><img src=image-1.png alt="Attetion is all your needä¸­çš„æ¶æ„"></p><ul><li>Embeddingï¼ˆåµŒå…¥å‘é‡ï¼‰ï¼šå°†è¾“å…¥æ–‡å­—è½¬åŒ–æˆæ•°å­—ï½œæ–‡å­—å‘é‡åŒ–</li></ul><pre><code class=language-json>Token Embedding Look-Up Table:
               0         1         2         3         4         5         6         7         8         9   ...        54        55        56        57        58        59        60        61        62        63
0       0.625765  0.025510  0.954514  0.064349 -0.502401 -0.202555 -1.567081 -1.097956  0.235958 -0.239778  ...  0.420812  0.277596  0.778898  1.533269  1.609736 -0.403228 -0.274928  1.473840  0.068826  1.332708
1      -0.497006  0.465756 -0.257259 -1.067259  0.835319 -1.956048 -0.800265 -0.504499 -1.426664  0.905942  ...  0.008287 -0.252325 -0.657626  0.318449 -0.549586 -1.464924 -0.557690 -0.693927 -0.325247  1.243933
2       1.347121  1.690980 -0.124446 -1.682366  1.134614 -0.082384  0.289316  0.835773  0.306655 -0.747233  ...  0.543340 -0.843840 -0.687481  2.138219  0.511412  1.219090  0.097527 -0.978587 -0.432050 -1.493750
3       1.078523 -0.614952 -0.458853  0.567482  0.095883 -1.569957  0.373957 -0.142067 -1.242306 -0.961821  ... -0.882441  0.638720  1.119174 -1.907924 -0.527563  1.080655 -2.215207  0.203201 -1.115814 -1.258691
4       0.814849 -0.064297  1.423653  0.261726 -0.133177  0.211893  1.449790  3.055426 -1.783010 -0.832339  ...  0.665415  0.723436 -1.318454  0.785860 -1.150111  1.313207 -0.334949  0.149743  1.306531 -0.046524
...          ...       ...       ...       ...       ...       ...       ...       ...       ...       ...  ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...
100064 -0.898191 -1.906910 -0.906910  1.838532  2.121814 -1.654444  0.082778  0.064536  0.345121  0.262247  ...  0.438956  0.163314  0.491996  1.721039 -0.124316  1.228242  0.368963  1.058280  0.406413 -0.326223
100065  1.354992 -1.203096 -2.184551 -1.745679 -0.005853 -0.860506  1.010784  0.355051 -1.489120 -1.936192  ...  1.354665 -1.338872 -0.263905  0.284906  0.202743 -0.487176 -0.421959  0.490739 -1.056457  2.636806
100066 -0.436116  0.450023 -1.381522  0.625508  0.415576  0.628877 -0.595811 -1.074244 -1.512645 -2.027422  ...  0.436522  0.068974  1.305852  0.005790 -0.583766 -0.797004  0.144952 -0.279772  1.522029 -0.629672
100067  0.147102  0.578953 -0.668165 -0.011443  0.236621  0.348374 -0.706088  1.368070 -1.428709 -0.620189  ...  1.130942 -0.739860 -1.546209 -1.475937 -0.145684 -1.744829  0.637790 -1.064455  1.290440 -1.110520
100068  0.415268 -0.345575  0.441546 -0.579085  1.110969 -1.303691  0.143943 -0.714082 -1.426512  1.646982  ... -2.502535  1.409418  0.159812 -0.911323  0.856282 -0.404213 -0.012741  1.333426  0.372255  0.722526

[100,069 rows x 64 columns]
</code></pre><p>è¿™é‡Œæ¯ä¸€ä¸ªè¡Œä»£è¡¨ä¸€ä¸ªTokenï¼Œè€Œæ¯ä¸ªç»´åº¦è¡¨ç¤ºä¸€ç§åœºæ™¯ï¼ˆæ¯”å¦‚è¿™ä¸ªè¯çš„å½¢å®¹è¯ã€åŠ¨è¯ã€åæ¬¡etcï¼‰</p><ul><li>Positional-Encodingï¼ˆä½ç½®ä¿¡æ¯-åµŒå…¥å‘é‡ï¼‰ï¼šè¾“å…¥æ–‡å­—ç›¸å¯¹ä½ç½®å…³ç³» ï½œ å‘é‡æ·»åŠ ä½ç½®ä¿¡æ¯</li></ul><p>ä½ç½®çŸ©é˜µåº”è¯¥æ˜¯ä¸€ä¸ªå’Œè¾“å…¥çŸ©é˜µæƒ³åŒç»´åº¦çš„çŸ©é˜µï¼Œåé¢éœ€è¦ç›¸åŠ </p><pre><code class=language-json>Position Embedding Look-Up Table:
          0         1         2         3         4         5         6         7         8         9   ...        54        55        56        57        58        59        60        61        62        63
0   0.000000  1.000000  0.000000  1.000000  0.000000  1.000000  0.000000  1.000000  0.000000  1.000000  ...  0.000000  1.000000  0.000000  1.000000  0.000000  1.000000  0.000000  1.000000  0.000000  1.000000
1   0.841471  0.540302  0.681561  0.731761  0.533168  0.846009  0.409309  0.912396  0.310984  0.950415  ...  0.000422  1.000000  0.000316  1.000000  0.000237  1.000000  0.000178  1.000000  0.000133  1.000000
2   0.909297 -0.416147  0.997480  0.070948  0.902131  0.431463  0.746904  0.664932  0.591127  0.806578  ...  0.000843  1.000000  0.000632  1.000000  0.000474  1.000000  0.000356  1.000000  0.000267  1.000000
3   0.141120 -0.989992  0.778273 -0.627927  0.993253 -0.115966  0.953635  0.300967  0.812649  0.582754  ...  0.001265  0.999999  0.000949  1.000000  0.000711  1.000000  0.000533  1.000000  0.000400  1.000000
4  -0.756802 -0.653644  0.141539 -0.989933  0.778472 -0.627680  0.993281 -0.115730  0.953581  0.301137  ...  0.001687  0.999999  0.001265  0.999999  0.000949  1.000000  0.000711  1.000000  0.000533  1.000000
5  -0.958924  0.283662 -0.571127 -0.820862  0.323935 -0.946079  0.858896 -0.512150  0.999947 -0.010342  ...  0.002108  0.999998  0.001581  0.999999  0.001186  0.999999  0.000889  1.000000  0.000667  1.000000
6  -0.279415  0.960170 -0.977396 -0.211416 -0.230368 -0.973104  0.574026 -0.818837  0.947148 -0.320796  ...  0.002530  0.999997  0.001897  0.999998  0.001423  0.999999  0.001067  0.999999  0.000800  1.000000
7   0.656987  0.753902 -0.859313  0.511449 -0.713721 -0.700430  0.188581 -0.982058  0.800422 -0.599437  ...  0.002952  0.999996  0.002214  0.999998  0.001660  0.999999  0.001245  0.999999  0.000933  1.000000
8   0.989358 -0.145500 -0.280228  0.959933 -0.977262 -0.212036 -0.229904 -0.973213  0.574318 -0.818632  ...  0.003374  0.999994  0.002530  0.999997  0.001897  0.999998  0.001423  0.999999  0.001067  0.999999
9   0.412118 -0.911130  0.449194  0.893434 -0.939824  0.341660 -0.608108 -0.793854  0.291259 -0.956644  ...  0.003795  0.999993  0.002846  0.999996  0.002134  0.999998  0.001600  0.999999  0.001200  0.999999
10 -0.544021 -0.839072  0.937633  0.347628 -0.612937  0.790132 -0.879767 -0.475405 -0.020684 -0.999786  ...  0.004217  0.999991  0.003162  0.999995  0.002371  0.999997  0.001778  0.999998  0.001334  0.999999
11 -0.999990  0.004426  0.923052 -0.384674 -0.097276  0.995257 -0.997283 -0.073661 -0.330575 -0.943780  ...  0.004639  0.999989  0.003478  0.999994  0.002609  0.999997  0.001956  0.999998  0.001467  0.999999
12 -0.536573  0.843854  0.413275 -0.910606  0.448343  0.893862 -0.940067  0.340989 -0.607683 -0.794179  ...  0.005060  0.999987  0.003795  0.999993  0.002846  0.999996  0.002134  0.999998  0.001600  0.999999
13  0.420167  0.907447 -0.318216 -0.948018  0.855881  0.517173 -0.718144  0.695895 -0.824528 -0.565821  ...  0.005482  0.999985  0.004111  0.999992  0.003083  0.999995  0.002312  0.999997  0.001734  0.999998
14  0.990607  0.136737 -0.878990 -0.476839  0.999823 -0.018796 -0.370395  0.928874 -0.959605 -0.281349  ...  0.005904  0.999983  0.004427  0.999990  0.003320  0.999995  0.002490  0.999997  0.001867  0.999998
15  0.650288 -0.759688 -0.968206  0.250154  0.835838 -0.548975  0.042249  0.999107 -0.999519  0.031022  ...  0.006325  0.999980  0.004743  0.999989  0.003557  0.999994  0.002667  0.999996  0.002000  0.999998

[16 rows x 64 columns]
</code></pre><ul><li>Multi-Head Attentionï¼šå¾—åˆ°æ¯ä¸ªè¯ç›¸å¯¹äºå…¶ä»–è¯çš„ä¸€ä¸ªæ¦‚ç‡è¡¨</li></ul><p><img src=image.png alt=çŸ©é˜µä¹˜æ³•></p><p><img src=image-1.png alt="Transformeræ¶æ„ä¸­çš„Multi-Head Attentionæœºåˆ¶"></p><p>QçŸ©é˜µå’ŒKçŸ©é˜µç›¸ä¹˜ï¼Œè¡¨ç¤ºæ¯ä¸ªå­—ç¬¦ç›¸å¯¹äºå…¶ä»–å­—ç¬¦çš„ç›¸ä¼¼åº¦ï¼Œå¾—åˆ°çš„ç»“æœçŸ©é˜µä¸­ï¼Œå€¼è¶Šå¤§ç›¸ä¼¼åº¦è¶Šé«˜</p><ul><li>Normalization Layerï¼šå°†æ¦‚ç‡åˆ†å¸ƒæ•´ä½“å‘ä¸‹å¹³ç§»ï¼Œå°çš„å€¼è¾ƒå°ç§»åŠ¨ï¼Œå¤§çš„å€¼è¾ƒå¤§ç§»åŠ¨ã€‚ä¹Ÿå«å½’ä¸€åŒ–</li></ul><p>è²Œä¼¼å’ŒSoftMaxæ˜¯ä¸€æ ·çš„ï¼Œä¹Ÿæ˜¯æŠŠå€¼è¿›è¡Œä¸€æ¬¡åˆç†æ¢³ç†</p><ul><li><p>Linearï¼šç»“åˆè¯æ±‡è¡¨ï¼Œå½¢æˆä¸€ä¸ªéå¸¸å¤§çš„çŸ©é˜µï¼ˆè¾“å…¥ä¸­åäººæ°‘å…±å’Œå›½ï¼Œå‡è®¾è¯æ±‡è¡¨ä¸º10000ï¼ŒçŸ©é˜µä¸º7*10000ï¼‰å¯ä»¥æ‰¾å‡ºæ¦‚ç‡æœ€å¤§çš„å­—ç¬¦</p><p>å‰ä¸€æ­¥è¾“å‡ºçš„16*64çŸ©é˜µå’Œ100000è¯æ±‡è¡¨çŸ©é˜µç›¸ä¹˜ï¼Œå¾—åˆ°çš„ç»“æœå°±æ˜¯æ¯ä¸ªtokenå’Œè¯æ±‡è¡¨å€¼çš„æ¦‚ç‡å€¼ã€‚æ¯ä¸ªtokenå¯¹åº”çš„æœ€å¤§å€¼å°±æ˜¯æœ€æœ‰å¯èƒ½è¢«é¢„æµ‹å‡ºç°çš„å•è¯</p></li></ul><h3 id=pre-training-é¢„è®­ç»ƒ>Pre-Training é¢„è®­ç»ƒ</h3><p>é¢„è®­ç»ƒä¹‹åå®é™…åªèƒ½è¾¾åˆ°æ–‡æœ¬å®Œæˆå™¨compeleterï¼Œé€šè¿‡Fine-Tuningå’ŒRLHFæ‰èƒ½åšåˆ°ç”Ÿæˆå™¨generator</p><h3 id=fine-tuning-å¾®è°ƒ>Fine-tuning å¾®è°ƒ</h3><h3 id=rlhf-å¼ºåŒ–å­¦ä¹ >RLHF å¼ºåŒ–å­¦ä¹ </h3><p>Reinforce Learning from Human Feedback</p><p>æ¨¡å‹ç»™å‡ºçš„å¤šä¸ªç»“æœåé¦ˆç»™ç”¨æˆ·ï¼Œç”¨æˆ·é€šè¿‡é€‰æ‹©æœ€åˆé€‚çš„ä¸€é¡¹æ¥ä¿®æ”¹æ¨¡å‹å‚æ•°</p><h3 id=prompt-engineering>Prompt Engineering</h3><h2 id=llm-ç¼–ç è¿‡ç¨‹>LLM-ç¼–ç è¿‡ç¨‹ğŸ§‘â€ğŸ’»</h2><h3 id=1-æ„é€ è¾“å…¥å¼ é‡>1. æ„é€ è¾“å…¥å¼ é‡</h3><p>ä¸‹è½½æ•°æ®é›†åˆ°æœ¬åœ°(å¯é€‰)</p><h4 id=ä¸‹è½½å¹¶å¯¼å…¥å¿…è¦åŒ…>ä¸‹è½½å¹¶å¯¼å…¥å¿…è¦åŒ…</h4><pre><code class=language-python>import torch    // æ‰‹åŠ¨å®‰è£…
import torch.nn as nn
import torch.nn.Functional as F
import os       // å†…ç½®æ¨¡å—
import request  // æ‰‹åŠ¨å®‰è£…  2.32.4
</code></pre><h4 id=è·å–æ•°æ®é›†>è·å–æ•°æ®é›†</h4><pre><code class=language-python># å¦‚æœæ²¡æœ‰æ–‡ä»¶ è¯·æ±‚æ•°æ®é›†
if not os.path.exists(&quot;sales-textbooks.txt&quot;):
    url = &quot;https://huggingface.co/datasets/goendalf666/sales-textbook_for_convincing_and_selling/resolve/main/sales_textbook.txt?download=true&quot;
    with open(&quot;sales-textbooks.txt&quot;,&quot;wb&quot;) as f:
        f.write(requests.get(url).content)

with open(&quot;sales-textbooks.txt&quot;,&quot;r&quot;) as f:
    text = f.read()
    print(&quot;ğŸš€ text:&quot;,text)
</code></pre><ol><li>TokenizeåŒ–</li></ol><p>å‡†å¤‡å¥½æ•°æ®é›†ä¹‹åï¼Œéœ€è¦å°†æ‹¿åˆ°çš„æ•°æ®é›†è¿›è¡ŒTokenizeåŒ–</p><p>Tokenizeçš„æ–¹æ³•æ¯”è¾ƒå¤šï¼š</p><h4 id=å®šä¹‰è¶…å‚æ•°>å®šä¹‰è¶…å‚æ•°</h4><pre><code class=language-python># hyperparameter
context_length = 16 
d_modal = 64
</code></pre><p>context_lengthæ˜¯è®­ç»ƒå¥å­çš„tokené•¿åº¦</p><h4 id=è®­ç»ƒé›†ä¸­æå–æ–‡æœ¬>è®­ç»ƒé›†ä¸­æå–æ–‡æœ¬</h4><pre><code class=language-python>data = train_data
idxs = torch.randint(low=0,high=len(data) - context_length,size=(batch_size,))
x_batch = torch.stack([data[idx:idx + context_length] for idx in idxs])
y_batch = torch.stack([data[idx+1:idx + context_length+1] for idx in idxs])
</code></pre><h4 id=åˆ›å»ºinputembeddingtable>åˆ›å»ºInputEmbeddingTable</h4><p>ç»™æ¯ä¸ªtokenæ‹“å±•ç»´åº¦ã€‚æ¯ä¸ªç»´åº¦ä»£è¡¨è¿™ä¸ªtokençš„ä¸€ç§å¯èƒ½æ€§</p><p>åˆå§‹åŒ–æ—¶ï¼Œæ¯ä¸ªtokenå¯¹åº”å¯èƒ½æ€§çš„åˆå§‹å€¼æ˜¯éšæœºå€¼ï¼Œéšç€è®­ç»ƒè¿™äº›éšæœºå€¼å˜å¾—å‡†ç¡®ä¸å†å”¯ä¸€</p><p>æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ä½¿ç”¨64ç»´ï¼Œè¡¨ç¤ºæ¯ä¸ªtokenæœ‰ç€64ç§å¯èƒ½æ€§</p><p>åˆ›å»ºä¹‹åçš„å¼ é‡å½¢æ€åº”è¯¥æ˜¯4âœ–ï¸16âœ–ï¸64</p><pre><code class=language-python>max_token_value = tokenized_text.max().item()
input_embedding_lookup_table = nn.Embedding(max_token_value+1,d_modal)
x_batch_embedding = input_embedding_lookup_table(x_batch)
y_batch_embedding = input_embedding_lookup_table(y_batch) # y_batch_embedding.shape = (4 * 16 * 64)
</code></pre><h4 id=åˆ›å»ºpositional-encoding>åˆ›å»ºpositional encoding</h4><p>å…ˆæ„é€ ä¸€ä¸ªå…¨0çŸ©é˜µï¼Œä¹‹åçŸ©é˜µå—é‡Œçš„å€¼æŒ‰ç…§ä¸‹é¢å…¬ç¤ºæ„é€ </p><blockquote><p>PE(pos, 2i) = sin(pos / 10000^(2i/d_model))</p><p>PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))</p></blockquote><pre><code class=language-python>import math
positinal_encoding_lookup_table = torch.zeros(context_length,d_modal)
position = torch.arange(0,context_length,dtype=torch.float).unsqueeze(1)
div_item = torch.exp(torch.arange(0,d_modal,2).float() * (-math.log(10000.0) / d_modal))

positinal_encoding_lookup_table[:,0::2] = torch.sin(position * div_item)
positinal_encoding_lookup_table[:,1::2] = torch.cos(position * div_item)
positinal_encoding_lookup_table = positinal_encoding_lookup_table.unsqueeze(0).expand(batch_size,-1,-1)

x = x_batch_embedding + positinal_encoding_lookup_table
y = y_batch_embedding + positinal_encoding_lookup_table
</code></pre><p><img src=image-2.png alt=ç­‰å¾…æœºå™¨å­¦ä¹ æ›´æ–°æƒé‡å€¼çš„çŸ©é˜µ></p><h3 id=2-transformeræ¶æ„ç¼–å†™>2. Transformeræ¶æ„ç¼–å†™</h3><h4 id=get-qkv>Get Qã€Kã€V</h4><pre><code class=language-python>Wq = nn.Linear(d_modal,d_modal)
Wk = nn.Linear(d_modal,d_modal)
Wv = nn.Linear(d_modal,d_modal)

Q = Wq(x)
K = Wk(x)
V = Wv(x)

</code></pre><h4 id=calculate-qkv>Calculate QKV</h4><p><img src=image-3.png alt></p><pre><code class=language-python>mask = torch.triu(torch.ones(context_length,context_length),diagonal=1).bool()
output = output.masked_fill(mask,float('-inf'))
print(pd.DataFrame(output[0,0].detach().numpy()))
</code></pre><p><img src=image-4.png alt></p><h4 id=æ³¨æ„åŠ›æœºåˆ¶ä¸­æ·»åŠ mask>æ³¨æ„åŠ›æœºåˆ¶ä¸­æ·»åŠ mask</h4><pre><code class=language-python>mask = torch.triu(torch.ones(context_length,context_length),diagonal=1).bool()
output = output.masked_fill(mask,float('-inf'))
# print(pd.DataFrame(output[0,0].detach().numpy()))
</code></pre><h4 id=softmax>softmax</h4><pre><code class=language-python>attentionn_score = F.softmax(output,dim=1)
</code></pre><h4 id=attention--v>attention @ V</h4><pre><code class=language-python>## attention @ V
A = attentionn_score @ V
</code></pre><h4 id=concateate>concateate</h4><pre><code class=language-python>A = A.transpose(1,2).reshape(batch_size,-1,d_modal)
Wo = nn.Linear(d_modal,d_modal)
output = Wo(A)
</code></pre><h4 id=residual-connection-æ®‹å·®è¿æ¥>residual connection æ®‹å·®è¿æ¥</h4><pre><code class=language-python>output = output + layer_norm_output
</code></pre><h4 id=layernorm>LayerNorm</h4><blockquote><p>å–å‡ºå¼ é‡ä¸­çš„æŸä¸€ä¸ªTokenè¡Œï¼Œå‡å€¼é è¿‘0ï¼Œæ–¹å·®æ¥è¿‘1</p><p>å‡å€¼ï¼š$$\mu = \frac{1}{H} \sum_{i=1}^{H} x_i$$</p><p>æ–¹å·®ï¼š$$\sigma^2 = \frac{1}{H} \sum_{i=1}^{H} (x_i - \mu)^2$$</p></blockquote><p>å½’ä¸€åŒ–ï¼š</p><p>$$\hat{x}_i = \frac{x_i - \mu}{\sqrt{\sigma^2 + \epsilon}}$$</p><p>$$\epsilon
$$æ˜¯ä¸€ä¸ªæ— é™å°çš„æ•°ï¼Œé˜²æ­¢é™¤0</p><p><img src=image-5.png alt></p><p>é‡æ–°ç¼©æ”¾å’Œåç§»ï¼š</p><p>$$Y_i = \gamma \hat{x}_i + \beta$$</p><pre><code class=language-python>output = layer_norm(output)
</code></pre><h4 id=feedforward>FeedForward</h4><pre><code class=language-python>output = nn.Linear(d_modal,d_modal * 4)(layer_norm_output) # æ”¾å¤§ç»´åº¦
output = nn.ReLU()(output)                      # æ¿€æ´»å‡½æ•°å¤„ç†
output = nn.Linear(d_modal * 4,d_modal)(output)
</code></pre><h4 id=linearlayer>LinearLayer</h4><pre><code class=language-python>output = nn.Linear(d_modal,max_token_value+1)(output)
</code></pre><p><img src=image-6.png alt=æ¶æ„è¾“å‡º></p><h4 id=decoding-strategyinferenceè¿‡ç¨‹>Decoding Strategy(Inferenceè¿‡ç¨‹)</h4><p>æ¨ç†ç­–ç•¥ï¼š</p><ol><li><p>Greedy Search</p><p>æ°¸è¿œè¿”å›logitsæœ€é«˜çš„token</p><p>å¾ˆå¤šæ¨¡å‹ä»¥åŠPytorch<strong>é»˜è®¤ç­–ç•¥</strong></p><p><img src=image-7.png alt></p></li><li><p>Beam Search</p></li></ol><p>åŒæ—¶é€‰å–logitsæœ€å¤§çš„Top Nä¸ªBeamï¼Œç­‰å¾…æ‰€æœ‰Tokené¢„æµ‹ç»“æŸåï¼Œè¿”å›æ‰€æœ‰Token logitæ€»å’Œæœ€å¤§çš„ä¸€ä¸ªBeam</p><p>ç¼ºç‚¹ï¼šç­‰å¾…æ—¶é—´ä¹…ï¼›é¢å‘Cç«¯åœºæ™¯ä¸é€‚ç”¨ï¼Œç ”ç©¶ä½¿ç”¨æ›´åˆé€‚</p><p><img src alt="Beam Search"></p><ul><li><p>TOP- K Sampling</p><p><strong>é€‰å–</strong>éšæœºKä¸ªæœ€é«˜æ¦‚ç‡å–æ ·</p></li><li><p>TOP- P Sampling</p></li></ul><p><strong>é€‰å–</strong>è¶…è¿‡é˜ˆå€¼</p><ul><li>Temperature</li></ul><p>GreadySearch + TOP-P[Optional] + Temperatureæ˜¯ç›®å‰æœ€ä¸»æµçš„ç­–ç•¥ç»„åˆ</p><p><img src alt=temperatureå¯¹probabilityçš„å½±å“></p><p><img src alt=modelå‚æ•°è®¾ç½®></p><h3 id=gpt35è¡¥å……çŸ¥è¯†>GPT3.5ï¼ˆè¡¥å……çŸ¥è¯†ï¼‰</h3><p>transformer_blockï¼š12</p><h3 id=3-ç”¨æ¨¡å‹è®­ç»ƒå°è¯´é¡¹ç›®>3. ç”¨æ¨¡å‹è®­ç»ƒå°è¯´é¡¹ç›®</h3><p>A. è®­ç»ƒæ¨¡å‹</p><pre><code class=language-python>âš¡ ~/LLM-Novel python3 train-model.py
æ•°æ®é›†åˆè®¡æœ‰ 16,397,115 tokens
Step: 0 Training Loss: 11.719 Validation Loss: 11.705
Step: 20 Training Loss: 6.159 Validation Loss: 6.228
Step: 40 Training Loss: 6.086 Validation Loss: 6.108
Step: 60 Training Loss: 6.002 Validation Loss: 6.034
Step: 80 Training Loss: 5.517 Validation Loss: 5.569
Step: 100 Training Loss: 5.1 Validation Loss: 5.135
Step: 120 Training Loss: 4.899 Validation Loss: 4.962
</code></pre><p>B. æ‰“å°æ¨¡å‹å‚æ•°</p><pre><code class=language-python>âš¡ ~/LLM-Novel python3 show-parameter.py 
æ¨¡å‹å‚æ•°é‡ä¸º:140573600
</code></pre><p>C. æ¨æ–­</p><pre><code class=language-python>âš¡ ~/LLM-Novel python3 Inference.py 
---------------
Liu Feng was born inï¼Œå°±åƒè¿™ä¸–ç•Œä¸­çš„ä¸€ä¸ªæ’æ˜Ÿçƒä¸€æ ·å­˜åœ¨ã€‚è¦çŸ¥é“ï¼Œåˆ°ç›®å‰ä¸ºæ­¢ï¼Œä¸ä¼šæœ‰å…¶å®ƒä½“ç³»ï¼Œï¼Œåˆæ€ä¹ˆèƒ½åœ¨é‚£åœºåˆå‘¢ï¼Ÿæ­£æ˜¯ä¸ºäº†ç ”ç©¶å®ƒçš„ç»„ç»‡ ï¼Œå¯¹äºç²¾åŠ›æ„æˆçƒ­ä½“ç§¯ç±»åŠä¸ªç‰©ç§æ–°å½¢çš„èƒ½åŠ›ã€‚å®ƒä¸€ç›´ä»¥ä¸ºä¼šç”¨ä»€ä¹ˆæ ·å­ï¼Œä½†æ˜¯å´æ²¡èƒ½ç”¨äºå­è¿è½¬ã€‚å®ƒæ¯ä¸ªæŠ€æœ¯ä¸Šéƒ½èƒ½å‘ç”Ÿæ•°ç™¾ä¸‡å¹´æ—¶é—´ã€‚åœ¨æ—¶é—´ç™¾ä¸‡å¯Œï¿½é‡Œï¼Œäººç±»å¹¶ä¸å­˜åœ¨æ ä¸Šæ˜Ÿé™…ä¹‹ç±»çš„æ˜Ÿç©ºç‰©ï¼Œå› ä¸ºä¸èƒ½å¼•äº§ç”Ÿæ°”å€™çš„å·¨å¤§é›¶æ— æ¯”ï¼Œä½†å®ƒä»¬çš„çµé­‚æ€»ä¼šå¤±å»æ— æ³•æ€è€ƒçš„å…³ç³»ã€‚
åœ¨åœ°çƒä½œåˆ†å…¬å…±å®ƒä»¬ä¹‹é—´çš„è¡¨é¢ï¼Œæ¯ä¸€æ¶â€œé»‘æ¡¶å…ƒè€å¸½â€ç°åœ¨ä¸€ç¿»è¿‡å¤§åœ°ï¼Œä¾¿ä¼šåœä¸‹æ¥é˜¿ç€èœ‚çŠæ˜Ÿçƒä¸°è¡Œå‰çº¿çš„å°å‹å¼•åŠ›å°ï¼Œå¹¶ä¸”è«é•¿æ— æ¯”ï¼Œè¿™æ˜¯æ²¡æœ‰é›¨åŒ… çš„ã€å‹åŠ›æœç€ç™½æ¡¥ä¹‹å¤–çš„åœ°æ–¹é£è¡Œæ´é«˜é«˜å±‚ã€‚
å¯æ˜¯ï¼Œå¦ä¸€ä¸ªå¸¦ç€ç”Ÿç‰©ä½¿çš±çœ‰å¤´é—ªç€å…‰ï¼Œä¸œå‡ æ ¹æ‰‹æŒ‡ç€å¸½å­ï¼Œâ€œçœ‹æ¥ï¼Œè¿™æ˜¯ä½ ä»¬çš„è„šçƒé‚£ä¹ˆé«˜ï¼Œæš—ç¤ºç¤¼è²Œã€‚â€ä»–é—®é“ã€‚
â€œæ˜¯å•Šï¼å¼„è¿™çš„ï¼Ÿâ€â€”â€”ä»–æ„Ÿåˆ°çº›ä¸ä»ä¸åŠ¨æ‘‡å¤´ã€‚
ä»–ç¥äº†ç”²æ¿æŸ”è½¯çš„ç°çœ¼ï¼Œåƒç”·äººæ‰€ç¼“ç¼“ä¸å·²çš„ç…¦åœ°æ¼«æ¸¸çš„ã€‚å®ƒå°†æ„è¯†æ­»ï¼Œéšè—åœ¨ä¸æ„Ÿå…´è¶£çš„ä¸€åˆ»ã€‚ä»–æ”¾è¿‡æ‰€æœ‰è¢«è½¯ç¡¬åˆè¢‹å’Œç¯å…‰ç…§äº®ï¼Œå®ƒè‚šå­åˆšè¿‡äº†
</code></pre><p>D. å¾®è°ƒï¼ˆäºŒæ¬¡è®­ç»ƒï¼‰</p><p>ä¸Šé¢ä¸‰æ­¥å®Œæˆä¹‹åï¼Œæ¨¡å‹ä»…ä»…åªèƒ½åšåˆ°Tokençš„è‡ªæˆ‘å»¶ä¼¸ã€‚æ— æ³•åšåˆ°ä¸€é—®ä¸€ç­”æ¨¡å¼ï¼Œæƒ³è¦åšåˆ°ä¸€é—®ä¸€ç­”æ¨¡å¼éœ€è¦è¿›è¡Œ<strong>å¾®è°ƒ</strong>è¿‡ç¨‹ã€‚ç®€å•è¯´å¯¹æ¨¡å‹ä¸æ–­çš„è¾“å…¥<strong>å¤§é‡çš„Q&Aæµ‹è¯•ç”¨ä¾‹</strong></p><p>è¿™æ­¥éª¤å®Œæˆä¹‹åï¼Œè¾“å…¥â€œè¯·å¸®æˆ‘å†™ä¸ªå°è¯´â€ï¼Œæ¨¡å‹åˆ™ä¸ä¼šè¿›è¡Œæ¨æ–­å»¶ä¼¸è€Œæ˜¯å¯ä»¥å¯ä»¥ç›´æ¥å›ç­”æˆ‘çš„é—®é¢˜</p><p>å¾®è°ƒçš„ä»£ç å’Œè®­ç»ƒçš„ä»£ç å‡ ä¹ä¸€æ ·ï¼Œåªæ˜¯ä¸€äº›è¶…å‚æ•°ä¸ä¸€æ ·</p><p>å¦å¤–ï¼Œå¾®è°ƒçš„æ•°æ®é›†æ˜¯è¦æ¯”è®­ç»ƒè¦å°çš„</p><p>æ¨¡å‹æ–°çš„å¾®è°ƒæ–¹å¼ï¼š</p><ul><li><p>compile</p></li><li><p>lora</p></li></ul><pre><code class=language-python># å¾®è°ƒæ–‡ä»¶
import os
import sys
import pickle
from contextlib import nullcontext
import torch
import tiktoken
from aim import Run
from model import Model
import json

# Hyperparameters
batch_size = 8  # How many batches per training step
context_length = 128  # Length of the token chunk each batch
max_iters = 1000  # Total of training iterations &lt;- Change this to smaller number for testing
learning_rate = 1e-4  # 0.001
eval_interval = 10  # How often to evaluate
eval_iters = 10  # Number of iterations to average for evaluation
device = 'cuda' if torch.cuda.is_available() else 'cpu'  # Use GPU if it's available.
TORCH_SEED = 1337
torch.manual_seed(TORCH_SEED)

# å‡†å¤‡è®­ç»ƒæ•°æ®
with open('data/scifi-finetune.json', 'r') as file:
    alpaca = json.load(file)
    text = alpaca[1000:5001]

# print(text)
# sys.exit(0)

# Using TikToken (Same as GPT3) to tokenize the source text
encoding = tiktoken.get_encoding(&quot;cl100k_base&quot;)
tokenized_text = encoding.encode(str(text))
tokenized_text = torch.tensor(tokenized_text, dtype=torch.long, device=device)  # å°†77,919ä¸ªtokens è½¬æ¢åˆ°Pytorchå¼ é‡ä¸­

total_tokens = encoding.encode_ordinary(str(text))
print(f&quot;æ•°æ®é›†åˆè®¡æœ‰ {len(total_tokens):,} tokens&quot;)

# Split train and validation
train_size = int(len(tokenized_text) * 0.9)
train_data = tokenized_text[:train_size]
val_data = tokenized_text[train_size:]

# Initialize the model
model = Model()
model.load_state_dict(torch.load('model/model-scifi.pt'))
model.to(device)

# get batch
def get_batch(split: str):
    data = train_data if split == 'train' else val_data
    idxs = torch.randint(low=0, high=len(data) - context_length, size=(batch_size,))
    x = torch.stack([data[idx:idx + context_length] for idx in idxs]).to(device)
    y = torch.stack([data[idx + 1:idx + context_length + 1] for idx in idxs]).to(device)
    return x, y

# calculate the loss
@torch.no_grad()
def estimate_loss():
    out = {}
    model.eval()
    for split in ['train', 'valid']:
        losses = torch.zeros(eval_iters)
        for k in range(eval_iters):
            x_batch, y_batch = get_batch(split)
            logits, loss = model(x_batch, y_batch)
            losses[k] = loss.item()
        out[split] = losses.mean()
    model.train()
    return out

# Create the optimizer
optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)
tracked_losses = list()
for step in range(max_iters):
    if step % eval_iters == 0 or step == max_iters - 1:
        losses = estimate_loss()
        tracked_losses.append(losses)
        print('Step:', step, 'Training Loss:', round(losses['train'].item(), 3), 'Validation Loss:', round(losses['valid'].item(), 3))

    xb, yb = get_batch('train')
    logits, loss = model(xb, yb)
    optimizer.zero_grad(set_to_none=True)
    loss.backward()
    optimizer.step()

# Save the model
torch.save(model.state_dict(), 'model/model-scifi-finetune.pt')
</code></pre><h3 id=4-stable-diffusionç”Ÿæˆå›¾ç‰‡>4. Stable Diffusionç”Ÿæˆå›¾ç‰‡</h3><p>ä¹Ÿå«æ–‡ç”Ÿå›¾ç‰‡æŠ€æœ¯ã€‚æ˜¯Stability.aiå…¬å¸åŸºäºTransformeræ¶æ„é¢„è®­ç»ƒçš„å¤§æ¨¡å‹</p><h4 id=æœ€æ–°æ¨¡å‹ç®€ä»‹>æœ€æ–°æ¨¡å‹ç®€ä»‹</h4><h4 id=ç¬¬ä¸‰æ–¹æ¨¡å‹ä½¿ç”¨æ–¹å¼>ç¬¬ä¸‰æ–¹æ¨¡å‹ä½¿ç”¨æ–¹å¼</h4><pre><code class=language-python># æœ¬é¡¹ç›®ä½¿ç”¨æ¥è‡ªHuggingFaceå·²é¢„è®­ç»ƒå¥½çš„æ¨¡å‹

from diffusers.pipelines.auto_pipeline import AutoPipelineForText2Image
from PIL import Image
import time
import torch

start_time = time.time()
pipe = AutoPipelineForText2Image.from_pretrained(&quot;stabilityai/sdxl-turbo&quot;, torch_dtype=torch.float16, variant=&quot;fp16&quot;)
pipe.to(&quot;cuda&quot;)

prompt = &quot;ç”»ä¸€ä¸ªå¤§æµ·&quot;
image = pipe(prompt=prompt, num_inference_steps=1, guidance_scale=0.0).images[0]
image.save('test2.png')

end_time = time.time()

gap_time = end_time - start_time
print(&quot;script execution time:&quot;,gap_time,&quot; seconds&quot;)
</code></pre><h4 id=æ¨¡å‹è¯„ä»·æŒ‡æ ‡--scalling-law>æ¨¡å‹è¯„ä»·æŒ‡æ ‡ & Scalling Law</h4><p>N - æ¨¡å‹å‚æ•°é‡ï¼ˆWeight and Biasï¼‰</p><p>D - è®­ç»ƒæ•°æ®é‡</p><p><img src alt="è®¡ç®—Losså…¬å¼ Deepmindç‰ˆ"></p><blockquote><p>Chinchilla Scalling Law:</p><ol><li><p><strong>æ•¸æ“šé‡ï¼ˆTokensæ•¸ï¼‰æ‡‰è©²è¦ç´„ç­‰æ–¼æ¨¡å‹åƒæ•¸é‡çš„20å€</strong></p></li><li><p><strong>ä¸¦ä¸”æ•¸æ“šé‡è·Ÿæ¨¡å‹åƒæ•¸é‡è¦åŒæ¯”æ”¾å¤§</strong>ï¼ˆEx: æ¨¡å‹æ”¾å¤§ä¸€å€ï¼Œæ•¸æ“šä¹Ÿè¦è·Ÿè‘—å¢åŠ ä¸€å€ï¼‰</p></li><li><p><a href=https://lifearchitect.ai/chinchilla/>Chinchilla data-optimal scaling laws: In plain English: In plain English</a></p></li></ol></blockquote><p><img src alt=è®¡ç®—é‡å’Œæ¨¡å‹å°ºå¯¸å…³ç³»></p><p><img src alt=DeepMind></p><p><img src alt="Scalling Lawè§„å¾‹"></p><p><img src alt></p><h2 id=llm-ç†è®ºè¿›é˜¶>LLM-ç†è®ºè¿›é˜¶ğŸ¥³</h2><h3 id=linear-tranformation>Linear Tranformation</h3><p><a href=https://pcnqdohorvbp.feishu.cn/wiki/Ma0qw4dEriEcXjkhh9BchYtanFh#share-OMkndoRAFok6jExzZJLcHD4MnAb>å¤§æ¨¡å‹- LLM</a></p><p><img src alt=Wq(X)></p><p>ä¸­é—´çŸ©é˜µï¼Œ$$512 \times 2$$ä¸­çš„2ä»£è¡¨<code>context-length</code>,æ¯”å¦‚â€œè‹¹æœâ€äºŒå­—</p></article><div class=my-4><a href=https://yaoyuanArtemis.github.io/tags/llm/ class="inline-block bg-tertiary-bg text-sm rounded px-3 py-1 my-1 me-2 hover:text-eureka">#LLM</a>
<a href=https://yaoyuanArtemis.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/ class="inline-block bg-tertiary-bg text-sm rounded px-3 py-1 my-1 me-2 hover:text-eureka">#äººå·¥æ™ºèƒ½</a></div><div class=py-2><div class="my-8 flex flex-col items-center md:flex-row"><a href=https://yaoyuanArtemis.github.io/authors/example-author/ class="md:me-4 text-primary-text h-24 w-24"><img src=https://raw.githubusercontent.com/yaoyuanArtemis/imgages/main/avatar.png class="bg-primary-bg w-full rounded-full" alt=Avatar></a><div class="mt-4 w-full md:mt-0 md:w-auto"><a href=https://yaoyuanArtemis.github.io/authors/example-author/ class="mb-2 block border-b pb-1 text-lg font-bold"><h3>Liu Feng</h3></a><span class="block pb-2">Aenean vel bibendum quam. Aliquam at mollis quam. Proin efficitur.</span>
<a href=yaoyuan.lf@gmail.com class=me-2><i class="fas fa-envelope"></i></a>
<a href="https://scholar.google.com/citations?user=0O_0fFAAAAAJ" class=me-2><i class="fab fa-google"></i></a>
<a href=https://github.com/yaoyuanArtemis class=me-2><i class="fab fa-github"></i></a>
<a href=public/images/resume.pdf class=me-2><i class="fas fa-file-pdf"></i></a></div></div></div><div class="-mx-2 mt-4 flex flex-col border-t px-2 pt-4 md:flex-row md:justify-between"><div><span class="text-primary-text block font-bold">Previous</span>
<a href=https://yaoyuanArtemis.github.io/posts/usehugo/ class=block>Hugo</a></div><div class="mt-4 md:mt-0 md:text-right"></div></div></div><div class=col-span-2><div class="bg-primary-bg
prose sticky top-16 z-10 hidden px-6 py-4 lg:block"><h3>On This Page</h3></div><div class="sticky-toc hidden px-6 pb-6 lg:block"><nav id=TableOfContents><ul><li><a href=#llm-åŸç†>LLM-åŸç†ğŸ˜ˆ</a><ul><li><a href=#èƒŒæ™¯>èƒŒæ™¯</a></li><li><a href=#æ¨¡å‹æ¶æ„>æ¨¡å‹æ¶æ„</a></li><li><a href=#pre-training-é¢„è®­ç»ƒ>Pre-Training é¢„è®­ç»ƒ</a></li><li><a href=#fine-tuning-å¾®è°ƒ>Fine-tuning å¾®è°ƒ</a></li><li><a href=#rlhf-å¼ºåŒ–å­¦ä¹ >RLHF å¼ºåŒ–å­¦ä¹ </a></li><li><a href=#prompt-engineering>Prompt Engineering</a></li></ul></li><li><a href=#llm-ç¼–ç è¿‡ç¨‹>LLM-ç¼–ç è¿‡ç¨‹ğŸ§‘â€ğŸ’»</a><ul><li><a href=#1-æ„é€ è¾“å…¥å¼ é‡>1. æ„é€ è¾“å…¥å¼ é‡</a><ul><li><a href=#ä¸‹è½½å¹¶å¯¼å…¥å¿…è¦åŒ…>ä¸‹è½½å¹¶å¯¼å…¥å¿…è¦åŒ…</a></li><li><a href=#è·å–æ•°æ®é›†>è·å–æ•°æ®é›†</a></li><li><a href=#å®šä¹‰è¶…å‚æ•°>å®šä¹‰è¶…å‚æ•°</a></li><li><a href=#è®­ç»ƒé›†ä¸­æå–æ–‡æœ¬>è®­ç»ƒé›†ä¸­æå–æ–‡æœ¬</a></li><li><a href=#åˆ›å»ºinputembeddingtable>åˆ›å»ºInputEmbeddingTable</a></li><li><a href=#åˆ›å»ºpositional-encoding>åˆ›å»ºpositional encoding</a></li></ul></li><li><a href=#2-transformeræ¶æ„ç¼–å†™>2. Transformeræ¶æ„ç¼–å†™</a><ul><li><a href=#get-qkv>Get Qã€Kã€V</a></li><li><a href=#calculate-qkv>Calculate QKV</a></li><li><a href=#æ³¨æ„åŠ›æœºåˆ¶ä¸­æ·»åŠ mask>æ³¨æ„åŠ›æœºåˆ¶ä¸­æ·»åŠ mask</a></li><li><a href=#softmax>softmax</a></li><li><a href=#attention--v>attention @ V</a></li><li><a href=#concateate>concateate</a></li><li><a href=#residual-connection-æ®‹å·®è¿æ¥>residual connection æ®‹å·®è¿æ¥</a></li><li><a href=#layernorm>LayerNorm</a></li><li><a href=#feedforward>FeedForward</a></li><li><a href=#linearlayer>LinearLayer</a></li><li><a href=#decoding-strategyinferenceè¿‡ç¨‹>Decoding Strategy(Inferenceè¿‡ç¨‹)</a></li></ul></li><li><a href=#gpt35è¡¥å……çŸ¥è¯†>GPT3.5ï¼ˆè¡¥å……çŸ¥è¯†ï¼‰</a></li><li><a href=#3-ç”¨æ¨¡å‹è®­ç»ƒå°è¯´é¡¹ç›®>3. ç”¨æ¨¡å‹è®­ç»ƒå°è¯´é¡¹ç›®</a></li><li><a href=#4-stable-diffusionç”Ÿæˆå›¾ç‰‡>4. Stable Diffusionç”Ÿæˆå›¾ç‰‡</a><ul><li><a href=#æœ€æ–°æ¨¡å‹ç®€ä»‹>æœ€æ–°æ¨¡å‹ç®€ä»‹</a></li><li><a href=#ç¬¬ä¸‰æ–¹æ¨¡å‹ä½¿ç”¨æ–¹å¼>ç¬¬ä¸‰æ–¹æ¨¡å‹ä½¿ç”¨æ–¹å¼</a></li><li><a href=#æ¨¡å‹è¯„ä»·æŒ‡æ ‡--scalling-law>æ¨¡å‹è¯„ä»·æŒ‡æ ‡ & Scalling Law</a></li></ul></li></ul></li><li><a href=#llm-ç†è®ºè¿›é˜¶>LLM-ç†è®ºè¿›é˜¶ğŸ¥³</a><ul><li><a href=#linear-tranformation>Linear Tranformation</a></li></ul></li></ul></nav></div><script>window.addEventListener("DOMContentLoaded",()=>{enableStickyToc()})</script></div></div><script>document.addEventListener("DOMContentLoaded",()=>{hljs.highlightAll()})</script></div></div></main><footer class=pl-scrollbar><div class="mx-auto w-full max-w-screen-xl"><div class="text-center p-6 pin-b"><p class="text-sm text-tertiary-text">Powered by the <a href=https://github.com/wangchucheng/hugo-eureka class=hover:text-eureka>Eureka</a> theme for <a href=https://gohugo.io class=hover:text-eureka>Hugo</a></p></div></div></footer></body></html>